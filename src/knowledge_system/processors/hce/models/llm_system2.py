"""
System 2 LLM wrapper that uses the centralized LLM adapter.

This provides a drop-in replacement for AnyLLM that routes all requests
through the System 2 LLM adapter for proper tracking and rate limiting.
"""

import asyncio
import json
import logging
from typing import Any

from ....config import get_settings
from ....core.llm_adapter import get_llm_adapter
from ....errors import ErrorCode, KnowledgeSystemError

logger = logging.getLogger(__name__)


class System2LLM:
    """
    System 2 compliant LLM interface that wraps the centralized adapter.

    Compatible with the AnyLLM interface but routes through System 2 infrastructure.
    """

    def __init__(
        self,
        provider: str = "ollama",
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        timeout: int | None = None,
    ):
        """
        Initialize System 2 LLM wrapper.

        Args:
            provider: LLM provider (ollama, openai, anthropic, google)
            model: Model name (defaults based on provider)
            temperature: Sampling temperature
            max_tokens: Max tokens to generate
            timeout: Request timeout in seconds (defaults from config)
        """
        self.provider = provider
        self.model = model or self._get_default_model(provider)
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.adapter = get_llm_adapter()

        # Get timeout from parameter or config
        if timeout is not None:
            self.timeout = timeout
        else:
            settings = get_settings()
            # Use local_config.timeout for local providers, otherwise default to 300s
            if provider in ["ollama", "local"]:
                self.timeout = settings.local_config.timeout
            else:
                self.timeout = 300  # 5 minutes for cloud APIs

        logger.debug(
            f"System2LLM initialized with {self.timeout}s timeout for {provider}"
        )

        # For compatibility with structured output
        self.supports_structured = provider == "ollama"

    def _get_default_model(self, provider: str) -> str:
        """Get default model for provider."""
        defaults = {
            "ollama": "qwen2.5:7b-instruct",
            "openai": "gpt-3.5-turbo",
            "anthropic": "claude-3-sonnet",
            "google": "gemini-pro",
        }
        return defaults.get(provider, "gpt-3.5-turbo")

    def set_job_run_id(self, job_run_id: str):
        """Set the current job run ID for tracking."""
        self.adapter.set_job_run_id(job_run_id)

    async def _complete_async(self, prompt: str, **kwargs) -> str:
        """Async completion through the adapter."""
        messages = [{"role": "user", "content": prompt}]

        # Allow kwargs to override instance defaults (e.g., temperature=0 for structured outputs)
        temperature = kwargs.pop("temperature", self.temperature)
        max_tokens = kwargs.pop("max_tokens", self.max_tokens)

        response = await self.adapter.complete_with_retry(
            provider=self.provider,
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs,
        )

        return response.get("content", "")

    def complete(self, prompt: str, **kwargs) -> str:
        """
        Generate a completion for the given prompt.

        Synchronous wrapper for compatibility with existing code.
        """
        import queue
        import threading

        # ALWAYS use thread-based approach when there might be an event loop
        # This is safer than trying to detect worker threads vs main thread
        def run_async_in_thread():
            """Run async function in a dedicated thread with its own event loop."""
            try:
                # Create fresh event loop for this thread
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    return loop.run_until_complete(
                        self._complete_async(prompt, **kwargs)
                    )
                finally:
                    loop.close()
                    asyncio.set_event_loop(None)
            except Exception as e:
                raise e

        # Try to detect if we're in an async context
        try:
            asyncio.get_running_loop()
            # There's a loop - use thread approach
            result_queue = queue.Queue()

            def thread_wrapper():
                try:
                    result = run_async_in_thread()
                    result_queue.put(("success", result))
                except Exception as e:
                    result_queue.put(("error", e))

            thread = threading.Thread(target=thread_wrapper, daemon=True)
            thread.start()
            thread.join(timeout=self.timeout)

            if thread.is_alive():
                raise KnowledgeSystemError(
                    f"LLM request timed out after {self.timeout} seconds",
                    ErrorCode.LLM_API_ERROR,
                )

            try:
                status, value = result_queue.get_nowait()
                if status == "error":
                    raise value
                return value
            except queue.Empty:
                raise KnowledgeSystemError(
                    "Thread completed but no result available", ErrorCode.LLM_API_ERROR
                )
        except RuntimeError:
            # No loop detected - safe to run directly
            return run_async_in_thread()

    async def _generate_json_async(self, prompt: str, **kwargs) -> dict[str, Any]:
        """Async JSON generation through the adapter."""
        # Add JSON instruction to prompt
        json_prompt = f"{prompt}\n\nRespond with valid JSON only."

        response_text = await self._complete_async(json_prompt, **kwargs)

        # Try to parse JSON
        try:
            return json.loads(response_text)
        except json.JSONDecodeError as e:
            # Try to extract JSON from the response
            import re

            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except json.JSONDecodeError:
                    pass

            logger.error(f"Failed to parse JSON response: {response_text[:200]}")
            raise KnowledgeSystemError(
                f"Invalid JSON response: {e}", ErrorCode.LLM_PARSE_ERROR
            ) from e

    def generate_json(self, prompt: str, **kwargs) -> dict[str, Any]:
        """
        Generate a JSON response for the given prompt.

        Synchronous wrapper for compatibility.
        """
        import queue
        import threading

        def run_async_in_thread():
            """Run async function in a dedicated thread with its own event loop."""
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    return loop.run_until_complete(
                        self._generate_json_async(prompt, **kwargs)
                    )
                finally:
                    loop.close()
                    asyncio.set_event_loop(None)
            except Exception as e:
                raise e

        # Try to detect if we're in an async context
        try:
            asyncio.get_running_loop()
            # There's a loop - use thread approach
            result_queue = queue.Queue()

            def thread_wrapper():
                try:
                    result = run_async_in_thread()
                    result_queue.put(("success", result))
                except Exception as e:
                    result_queue.put(("error", e))

            thread = threading.Thread(target=thread_wrapper, daemon=True)
            thread.start()
            thread.join(timeout=self.timeout)

            if thread.is_alive():
                raise KnowledgeSystemError(
                    f"LLM request timed out after {self.timeout} seconds",
                    ErrorCode.LLM_API_ERROR,
                )

            try:
                status, value = result_queue.get_nowait()
                if status == "error":
                    raise value
                return value
            except queue.Empty:
                raise KnowledgeSystemError(
                    "Thread completed but no result available", ErrorCode.LLM_API_ERROR
                )
        except RuntimeError:
            # No loop detected - safe to run directly
            return run_async_in_thread()

    async def _generate_structured_json_async(
        self, prompt: str, schema_name: str, **kwargs
    ) -> dict[str, Any]:
        """
        Generate JSON using fast JSON mode + robust repair logic.

        Strategy (optimized for speed):
        1. Use format="json" (5x faster than grammar mode)
        2. Parse + repair (fixes 95% of common LLM errors)
        3. If validation fails: log warning but return repaired version

        Performance:
        - JSON mode: ~4s per segment (vs ~24s with grammar mode)
        - Parallel efficiency: 81% (4x speedup with 5 workers)
        - Auto-repair success: 95%
        - Total speedup: ~5-6x faster end-to-end
        """
        # Use JSON mode for speed (no schema constraint at generation time)
        logger.debug(f"Generating JSON for {schema_name} (JSON mode + repair)")

        # Use temperature=0 for speed (greedy decoding, faster)
        # Repair logic will fix any errors from deterministic generation
        if "temperature" not in kwargs:
            kwargs["temperature"] = 0.0

        raw_response = await self._generate_json_async(prompt, format="json", **kwargs)

        # Parse response
        if isinstance(raw_response, str):
            try:
                parsed = json.loads(raw_response)
            except json.JSONDecodeError as e:
                logger.error(f"JSON parse error: {e}")
                # Return empty structure as fallback
                return {"claims": [], "jargon": [], "people": [], "mental_models": []}
        elif isinstance(raw_response, dict):
            parsed = raw_response
        else:
            logger.error(f"Unexpected response type: {type(raw_response)}")
            return {"claims": [], "jargon": [], "people": [], "mental_models": []}

        # Validate and repair
        from ..schema_validator import repair_and_validate_miner_output

        repaired, is_valid, errors = repair_and_validate_miner_output(parsed)

        if not is_valid:
            logger.warning(
                f"Schema validation failed for {schema_name} (after repair): {errors[:200]}"
            )
            # Continue with repaired version (95% of errors are fixed by repair)
        else:
            logger.debug(f"âœ“ {schema_name}: Valid JSON after repair")

        return repaired

    def generate_structured_json(
        self, prompt: str, schema_name: str, **kwargs
    ) -> dict[str, Any]:
        """
        Generate structured JSON following a schema (Ollama only).

        Falls back to regular JSON generation for other providers.
        """
        import queue
        import threading

        def run_async_in_thread():
            """Run async function in a dedicated thread with its own event loop."""
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    return loop.run_until_complete(
                        self._generate_structured_json_async(
                            prompt, schema_name, **kwargs
                        )
                    )
                finally:
                    loop.close()
                    asyncio.set_event_loop(None)
            except Exception as e:
                raise e

        # Try to detect if we're in an async context
        try:
            asyncio.get_running_loop()
            # There's a loop - use thread approach
            result_queue = queue.Queue()

            def thread_wrapper():
                try:
                    result = run_async_in_thread()
                    result_queue.put(("success", result))
                except Exception as e:
                    result_queue.put(("error", e))

            thread = threading.Thread(target=thread_wrapper, daemon=True)
            thread.start()
            thread.join(timeout=self.timeout)

            if thread.is_alive():
                raise KnowledgeSystemError(
                    f"LLM request timed out after {self.timeout} seconds",
                    ErrorCode.LLM_API_ERROR,
                )

            try:
                status, value = result_queue.get_nowait()
                if status == "error":
                    raise value
                return value
            except queue.Empty:
                raise KnowledgeSystemError(
                    "Thread completed but no result available", ErrorCode.LLM_API_ERROR
                )
        except RuntimeError:
            # No loop detected - safe to run directly
            return run_async_in_thread()

    def get_stats(self) -> dict[str, Any]:
        """Get LLM usage statistics."""
        return self.adapter.get_stats()


def create_system2_llm(
    provider: str = "ollama", model: str | None = None, **kwargs
) -> System2LLM:
    """Factory function to create a System 2 LLM instance."""
    return System2LLM(provider=provider, model=model, **kwargs)
