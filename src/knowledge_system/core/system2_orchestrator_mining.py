"""Mining integration for System2Orchestrator using UnifiedHCEPipeline."""

import logging
import os
from pathlib import Path
from typing import Any

from ..errors import ErrorCode, KnowledgeSystemError
from ..processors.hce.config_flex import PipelineConfigFlex, StageModelConfig
from ..processors.hce.types import EpisodeBundle, Segment
from ..processors.hce.unified_pipeline import UnifiedHCEPipeline

logger = logging.getLogger(__name__)


async def process_mine_with_unified_pipeline(
    orchestrator,  # System2Orchestrator instance
    source_id: str,
    config: dict[str, Any],
    checkpoint: dict[str, Any] | None,
    run_id: str,
) -> dict[str, Any]:
    """
    Process mining job using UnifiedHCEPipeline for parallel processing and rich data.

    Benefits over old approach:
    - 3-8x faster via parallel processing
    - Evidence spans with timestamps
    - Claim evaluation and A/B/C ranking
    - Relations between claims
    - Structured categories
    - Full checkpoint support for resumability

    Checkpoint structure:
    {
        "stage": "parsing" | "mining" | "storing" | "completed",
        "completed_segments": ["seg_0001", "seg_0002", ...],
        "pipeline_outputs_partial": {...},  # Partial results if interrupted
        "total_segments": 100
    }
    """

    try:
        # Check if already completed
        if checkpoint and checkpoint.get("stage") == "completed":
            logger.info(
                f"Job already completed (from checkpoint), returning cached results"
            )
            return checkpoint.get(
                "final_result", {"status": "succeeded", "output_id": source_id}
            )

        # 1. Load transcript segments (prefer DB over file)
        if orchestrator.progress_callback:
            orchestrator.progress_callback("loading", 0, source_id)

        # source_id is used directly (no episode_ prefix needed)

        # PRIORITY 1: Try to load segments from database (our own transcripts)
        whisper_segments = orchestrator._load_transcript_segments_from_db(source_id)

        segments = None
        if whisper_segments and len(whisper_segments) > 0:
            logger.info(
                f"‚úÖ Using database segments for {source_id} ({len(whisper_segments)} raw Whisper segments)"
            )

            # Re-chunk Whisper segments for efficient HCE processing
            if orchestrator.progress_callback:
                orchestrator.progress_callback("parsing", 3, source_id)

            segments = orchestrator._rechunk_whisper_segments(
                whisper_segments, source_id
            )
            logger.info(
                f"üì¶ Re-chunked into {len(segments)} optimized segments for HCE mining"
            )

        # FALLBACK: Parse from markdown file if DB segments not available
        if not segments:
            file_path = config.get("file_path")
            if not file_path:
                raise KnowledgeSystemError(
                    f"No segments in DB and no file_path provided for source {source_id}",
                    ErrorCode.INVALID_INPUT,
                )

            logger.info(
                f"‚ö†Ô∏è No DB segments found, falling back to parsing markdown file: {file_path}"
            )
            transcript_text = Path(file_path).read_text()

            if orchestrator.progress_callback:
                orchestrator.progress_callback("parsing", 5, source_id)

            segments = orchestrator._parse_transcript_to_segments(
                transcript_text, source_id
            )

        # Save checkpoint after parsing/loading
        if not (checkpoint and checkpoint.get("stage") in ["mining", "storing"]):
            orchestrator.save_checkpoint(
                run_id,
                {
                    "stage": "parsing",
                    "total_segments": len(segments),
                    "completed_segments": [],
                },
            )

        if not segments:
            logger.warning(f"No segments parsed from transcript for {source_id}")
            return {
                "status": "succeeded",
                "output_id": source_id,
                "result": {
                    "claims_extracted": 0,
                    "evidence_spans": 0,
                    "jargon_extracted": 0,
                    "people_extracted": 0,
                    "mental_models_extracted": 0,
                    "relations": 0,
                    "categories": 0,
                },
            }

        logger.info(f"üìÑ Parsed {len(segments)} segments for mining")

        # 3. Check for completed segments and filter if resuming
        completed_segment_ids = set()
        if checkpoint and checkpoint.get("stage") == "mining":
            completed_segment_ids = set(checkpoint.get("completed_segments", []))
            if completed_segment_ids:
                logger.info(
                    f"Resuming mining: {len(completed_segment_ids)}/{len(segments)} segments already completed"
                )
                # Filter to only unprocessed segments
                segments = [
                    s for s in segments if s.segment_id not in completed_segment_ids
                ]
                logger.info(f"Processing remaining {len(segments)} segments")

        # 3a. Fetch video metadata for evaluator/summary context
        video_metadata = None
        try:
            source_id = source_id
            video = orchestrator.db_service.get_source(source_id)
            if video:
                # Claim-centric schema doesn't have tags_json or video_chapters_json
                video_metadata = {
                    "title": video.title,
                    "description": video.description,
                    "uploader": video.uploader,
                    "upload_date": video.upload_date,
                    "duration_seconds": video.duration_seconds,
                    "tags": None,  # Not available in claim-centric schema
                    "chapters": None,  # Not available in claim-centric schema
                    "url": video.url,
                }
                logger.info(f"üì∫ Loaded video metadata: {video.title}")
        except Exception as e:
            logger.warning(f"Could not fetch video metadata: {e}")

        # 3b. Create EpisodeBundle with metadata
        episode_bundle = EpisodeBundle(
            episode_id=source_id, segments=segments, video_metadata=video_metadata
        )

        # 4. Configure HCE Pipeline
        miner_model = config.get("miner_model", "ollama:qwen2.5:7b-instruct")

        # Allow configuration of parallelization
        max_workers = config.get("max_workers", None)  # None = auto-calculate
        enable_parallel = config.get("enable_parallel_processing", True)

        hce_config = PipelineConfigFlex(
            models=StageModelConfig(
                miner=miner_model,
                judge=miner_model,  # Can be different model if needed
                flagship_judge=miner_model,  # Can be different model if needed
            ),
            max_workers=max_workers if enable_parallel else 1,
            enable_parallel_processing=enable_parallel,
            content_type=config.get(
                "content_type"
            ),  # Pass content type for prompt selection
        )

        logger.info(
            f"üîß HCE Config: miner={miner_model}, "
            f"parallel={'auto' if max_workers is None and enable_parallel else max_workers}, "
            f"run_id={run_id}"
        )

        # 5. Initialize UnifiedHCEPipeline
        pipeline = UnifiedHCEPipeline(hce_config)

        # 6. Create progress callback wrapper with checkpoint saving
        segment_counter = {"count": len(completed_segment_ids)}
        checkpoint_interval = max(1, len(segments) // 10)  # Save checkpoint every 10%

        def progress_wrapper(progress_obj):
            """Wrap pipeline progress to orchestrator format and save periodic checkpoints."""
            if orchestrator.progress_callback:
                # Pipeline reports SummarizationProgress objects
                # Map to orchestrator's callback format
                step = (
                    progress_obj.current_step
                    if hasattr(progress_obj, "current_step")
                    else "processing"
                )
                percent = (
                    progress_obj.file_percent
                    if hasattr(progress_obj, "file_percent")
                    else 0
                )
                # Map 0-100% to orchestrator's 10-95% range
                adjusted_percent = 10 + int(percent * 0.85)
                orchestrator.progress_callback(step, adjusted_percent, source_id)

            # Periodic checkpoint saving during mining
            # Note: This is a best-effort approach; actual segment completion
            # tracking would require deeper integration with parallel processor
            segment_counter["count"] += 1
            if segment_counter["count"] % checkpoint_interval == 0:
                try:
                    orchestrator.save_checkpoint(
                        run_id,
                        {
                            "stage": "mining",
                            "total_segments": checkpoint.get(
                                "total_segments", len(segments)
                            )
                            if checkpoint
                            else len(segments),
                            "completed_segments": list(completed_segment_ids),
                            "progress_percent": adjusted_percent
                            if orchestrator.progress_callback
                            else 0,
                        },
                    )
                    logger.debug(
                        f"Saved mining checkpoint at {segment_counter['count']} segments"
                    )
                except Exception as e:
                    logger.warning(f"Failed to save checkpoint: {e}")

        # 7. Process with full pipeline (mining + evaluation + categories)
        logger.info(f"üöÄ Starting UnifiedHCEPipeline for {len(segments)} segments")

        # Save checkpoint before starting mining
        orchestrator.save_checkpoint(
            run_id,
            {
                "stage": "mining",
                "total_segments": checkpoint.get("total_segments", len(segments))
                if checkpoint
                else len(segments),
                "completed_segments": list(completed_segment_ids),
            },
        )

        # Align adapter local concurrency with pipeline workers for this run
        try:
            effective_workers = hce_config.max_workers if enable_parallel else 1
            if effective_workers:
                os.environ["HCE_EFFECTIVE_MAX_WORKERS"] = str(effective_workers)
                logger.info(
                    f"Concurrency alignment: HCE_EFFECTIVE_MAX_WORKERS={effective_workers}"
                )
        except Exception:
            pass

        pipeline_outputs = pipeline.process(
            episode_bundle, progress_callback=progress_wrapper
        )

        logger.info(
            f"‚úÖ Pipeline complete: {len(pipeline_outputs.claims)} claims, "
            f"{sum(len(c.evidence) for c in pipeline_outputs.claims)} evidence spans, "
            f"{len(pipeline_outputs.relations)} relations, "
            f"{len(pipeline_outputs.structured_categories)} categories"
        )

        # 8. Store rich outputs to main database (claim-centric schema)
        if orchestrator.progress_callback:
            orchestrator.progress_callback("storing", 90, source_id)

        # Save checkpoint before storage
        orchestrator.save_checkpoint(
            run_id,
            {
                "stage": "storing",
                "total_segments": checkpoint.get("total_segments", len(segments))
                if checkpoint
                else len(segments),
                "completed_segments": list(completed_segment_ids),
            },
        )

        try:
            # Determine source_id (strip episode_ prefix if present)
            source_id = source_id

            # Use ClaimStore for claim-centric storage
            from ..database.claim_store import ClaimStore

            claim_store = ClaimStore(orchestrator.db_service)

            # CRITICAL: Store segments BEFORE storing claims
            # This ensures foreign key constraints are satisfied when storing evidence spans
            # Pass source_id and episode_title so the episode record can be created if needed
            episode_title = Path(file_path).stem
            claim_store.store_segments(
                source_id, segments, source_id=source_id, episode_title=episode_title
            )
            logger.info(f"üíæ Stored {len(segments)} segments for episode {source_id}")

            claim_store.upsert_pipeline_outputs(
                pipeline_outputs,
                source_id=source_id,
                source_type="episode",
                episode_title=Path(file_path).stem,
            )
            logger.info("üíæ Stored claims with evidence to claim-centric database")

            # Verify claims were actually written to database
            if orchestrator.progress_callback:
                orchestrator.progress_callback("verifying", 93, source_id)

            with orchestrator.db_service.get_session() as session:
                from ..database.models import Claim

                verified_claims = (
                    session.query(Claim).filter_by(source_id=source_id).count()
                )

            if verified_claims != len(pipeline_outputs.claims):
                raise KnowledgeSystemError(
                    f"Database verification failed: expected {len(pipeline_outputs.claims)} claims, found {verified_claims}",
                    ErrorCode.DATABASE_ERROR,
                )

            logger.info(
                f"‚úÖ Database verification passed: {verified_claims} claims stored"
            )

        except Exception as e:
            logger.error(f"‚ùå Database storage or verification failed: {e}")
            raise

        # 9. Summaries now stored in episodes table (via ClaimStore)
        if orchestrator.progress_callback:
            orchestrator.progress_callback("generating_summary", 96, source_id)

        # Note: Summary text (short_summary, long_summary) is already stored
        # in the episodes table by ClaimStore.upsert_pipeline_outputs()
        logger.info(f"üìã Episode summaries stored in episodes table")

        # 10. Generate summary markdown file
        summary_file_path = None
        try:
            from ..services.file_generation import FileGenerationService

            output_dir = config.get("output_dir")
            if output_dir:
                file_gen = FileGenerationService(output_dir=Path(output_dir))
            else:
                file_gen = FileGenerationService()

            summary_file_path = file_gen.generate_summary_markdown_from_pipeline(
                source_id, source_id, pipeline_outputs
            )

            if summary_file_path:
                logger.info(f"‚úÖ Summary file: {summary_file_path}")

        except Exception as e:
            logger.error(f"‚ùå Summary file generation failed: {e}")

        # 11. Finalize checkpoint
        if orchestrator.progress_callback:
            orchestrator.progress_callback("finalizing", 99, source_id)

        # 12. Build final results
        final_result = {
            "status": "succeeded",
            "output_id": source_id,
            "summary_file": str(summary_file_path) if summary_file_path else None,
            "result": {
                # Claim metrics
                "claims_extracted": len(pipeline_outputs.claims),
                "claims_tier_a": len(
                    [c for c in pipeline_outputs.claims if c.tier == "A"]
                ),
                "claims_tier_b": len(
                    [c for c in pipeline_outputs.claims if c.tier == "B"]
                ),
                "claims_tier_c": len(
                    [c for c in pipeline_outputs.claims if c.tier == "C"]
                ),
                "evidence_spans": sum(len(c.evidence) for c in pipeline_outputs.claims),
                # Entity metrics
                "jargon_extracted": len(pipeline_outputs.jargon),
                "people_extracted": len(pipeline_outputs.people),
                "mental_models_extracted": len(pipeline_outputs.concepts),
                # Rich data metrics
                "relations": len(pipeline_outputs.relations),
                "categories": len(pipeline_outputs.structured_categories),
                # Processing metrics
                "segments_processed": len(segments),
                "parallel_workers": hce_config.max_workers or "auto",
            },
        }

        # 13. Save final checkpoint marking job as completed
        orchestrator.save_checkpoint(
            run_id,
            {
                "stage": "completed",
                "total_segments": checkpoint.get("total_segments", len(segments))
                if checkpoint
                else len(segments),
                "completed_segments": list(completed_segment_ids),
                "final_result": final_result,
            },
        )

        logger.info(f"‚úÖ Mining job completed and checkpoint saved")

        return final_result

    except Exception as e:
        logger.error(f"‚ùå Mining failed for {source_id}: {e}")
        # Save error checkpoint for potential retry with partial results
        try:
            orchestrator.save_checkpoint(
                run_id,
                {
                    "stage": "failed",
                    "total_segments": checkpoint.get("total_segments", 0)
                    if checkpoint
                    else 0,
                    "completed_segments": list(completed_segment_ids)
                    if completed_segment_ids
                    else [],
                    "error": str(e),
                },
            )
        except Exception as checkpoint_error:
            logger.warning(f"Failed to save error checkpoint: {checkpoint_error}")
        raise
