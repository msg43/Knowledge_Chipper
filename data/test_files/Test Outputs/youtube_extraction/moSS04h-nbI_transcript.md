---
title: "YouTube Video moSS04h-nbI"
source: "https://www.youtube.com/watch?v=moSS04h-nbI"
video_id: "moSS04h-nbI"
language: "en"
type: "Auto-generated"
model: "YouTube Transcript"
device: "Web API"
fetched: "2025-08-23 12:49:31"
---

![Video Thumbnail](Thumbnails/moSS04h-nbI_thumbnail.jpg)

**ðŸŽ¥ [Watch on YouTube](https://www.youtube.com/watch?v=moSS04h-nbI)**

## Full Transcript

**00:12** welcome everyone to the Union Chapel and

**00:14** to intelligence squar it is so fabulous

**00:17** to be back with all of you in the hall

**00:19** not on a computer there are however

**00:21** people watching online so hello to you

**00:24** thank you for joining in the very fact

**00:27** that uh we're all here tonight is proof

**00:29** enough isn't it that we've already made

**00:31** at least one intelligent decision in a

**00:33** chaotic World added to which I haven't

**00:35** arrived with a suitcase full of wine or

**00:38** indeed a karaoke machine uh and I know

**00:41** we'll all be polite to the security

**00:43** staff and the cleaners um our decision

**00:46** making so far has been impeccable but uh

**00:49** this evening's conversation May reveal

**00:51** that perhaps not all of our decisions

**00:53** are as sound as we'd like to believe our

**00:56** confidence in our own expertise may be

**00:59** misplaced and our judgments may be

**01:01** flawed so without further Ado let me

**01:03** introduce our speakers Daniel kman

**01:06** probably doesn't need much of an

**01:07** introduction he's a Nobel Prize winner

**01:09** and the author of The hugely influential

**01:11** International bestseller thinking fast

**01:14** and slow and his day job is as professor

**01:16** of psychology and public affairs at

**01:18** Princeton University um Professor

**01:20** caraman is of course here to talk about

**01:23** his new book noise a flaw in human

**01:25** judgment and I'm delighted to say that

**01:27** we're also joined by one of his

**01:28** co-authors Olivier bourney who is

**01:31** Professor of strategy and business

**01:32** policy at H Paris previously he was a

**01:35** senior partner at McKenzie and he's also

**01:38** the author of you're about to make a

**01:39** terrible mistake well I hope we're not

**01:41** about to do that right now so um before

**01:45** we really get going I just want to

**01:46** remind you that I want all of you to get

**01:48** involved both in the hall and at home

**01:51** online uh there's a chance for lots of

**01:54** q&as at towards the end after about an

**01:56** hour or so there will be microphones

**01:58** down here at the front and up there

**02:01** please come forward I would love to hear

**02:03** from you all and if you're online there

**02:05** is a sort of a a bit at the bottom of

**02:08** the screen as it were uh you can ask

**02:10** questions in the in the Box just beneath

**02:12** the screen type them in we want to hear

**02:14** from you get thinking and uh I'll make

**02:17** sure there's plenty of time for

**02:18** questions from

**02:20** everyone let's get going then noise

**02:23** fresan basic definitions what is noise

**02:28** well noise has many meanings but the

**02:32** noise that we're talking about is

**02:34** Judgment

**02:35** noise and judgment noise is variability

**02:39** that shouldn't exist it's when people

**02:42** make judgments that should be identical

**02:44** because they're

**02:46** about the same subject but they turn out

**02:49** to be variable and and there is a lot of

**02:52** noise and our statement about this is

**02:55** that wherever there is Judgment there is

**02:57** noise and there is a lot more than you

**03:00** think and you know that's really what

**03:03** the book is about and there is more

**03:05** noise Than People expect to see and how

**03:09** is that different from what we think of

**03:11** as bias well to draw the distinction

**03:15** sharply between noise and bias we we

**03:17** have to see what we have to talk about

**03:19** what judgment is and the way we think

**03:22** about judgment is we think of judgment

**03:24** as a species of

**03:26** measurement where the measuring

**03:28** instrument is the human mind and like

**03:32** other measurements you deal with a

**03:35** subject an object topic and you assign

**03:39** that topic of value on a scale and this

**03:41** is what Jud the kind of judgment that we

**03:43** talk about is we talk about really

**03:46** judgments about specific

**03:49** topics now in measurement there is a

**03:53** theory of measurement that is a basic

**03:55** Theory you know that serves uh all the

**03:58** sciences and there is a of course a

**04:00** theory of Errors of measurement so when

**04:02** you measure you make take a physical

**04:05** measurement uh say of a line with a very

**04:09** very fine ruler and you take repeated

**04:12** measurements of the length of the same

**04:14** lines uh then what's going to happen is

**04:16** this you're not going to get the same

**04:19** result every time if the the ruler is

**04:23** fine enough uh and that variability that

**04:27** is noise but there is another kind of

**04:30** error and this is that every measurement

**04:33** is likely to be an error an overestimate

**04:36** or an underestimate and the average

**04:38** error is called bias and this is really

**04:42** those are the meanings of bias and noise

**04:45** that we use in discussing judgment so

**04:48** bias is an average systematic error and

**04:52** noise is just a variability of judgments

**04:55** and it's a variability that shouldn't

**04:57** exist so Olivier why are we all so so

**04:59** much more familiar with the idea of bias

**05:01** is it the sort of sexy cousin of noisy

**05:04** well it's sexier it's more visible so a

**05:08** way to think about the definition Danny

**05:10** gave which is perhaps more visual is to

**05:13** imagine that you've got a team of

**05:15** Shooters shooting at a Target they're

**05:18** all using the same rifle and they're all

**05:20** shooting at the same Target and you look

**05:22** at the Target after they all took their

**05:24** shot and if you see that they all hit

**05:27** the bullseye you say great they're

**05:28** accurate now if you see that they've all

**05:31** hit the same place but that place is not

**05:33** the bullseye you immediately say this

**05:37** calls for an explanation there must be a

**05:39** reason there's something wrong with the

**05:40** rifle or there is something wrong with

**05:42** the Target or somebody moved the Target

**05:44** or maybe the wind is blowing really hard

**05:47** you immediately look for a c that's why

**05:50** bias is sexier as you say right because

**05:53** bias is something you can explain you

**05:54** can point the finger at the reason why

**05:57** these people all made the same mistake

**06:00** if by contrast you see your five or six

**06:03** shooters shooting all over the place you

**06:06** just shrug and say Well they're not

**06:08** great marksmen but that doesn't give you

**06:11** an urge to explain so noise is more

**06:15** random by definition and is less easy to

**06:19** spot when you see it I'm still curious

**06:22** about why we haven't thought about this

**06:24** more systematically in the way that you

**06:26** do in the book is it that in a sense

**06:29** people in general just have too much

**06:31** confidence in their judgment in their

**06:33** decision making well you know it would

**06:36** be an exaggeration to say that people

**06:38** have not thought about it U people have

**06:40** looked at reliability of judgment for a

**06:43** long time what I think Justified writing

**06:47** the noise writing the book

**06:50** was with the pre well the prevalence

**06:54** just a sheer amount of noise and for

**06:56** that possibly an example would use

**06:59** absolutely why should we worry about it

**07:01** I'd like to tell the story of how the

**07:03** study began and and the study

**07:06** began number of years ago when I was

**07:08** doing a consulting job at uh at an

**07:12** insurance company and I had the idea of

**07:15** running what we now would call a noise

**07:17** audit but it's really a fairly standard

**07:20** exercise of

**07:22** constructing problems that were typical

**07:25** underwriting problems quite realistic

**07:28** and having multiple

**07:31** Underwriters put a dollar value on the

**07:34** same

**07:36** risk and of course you wouldn't expect

**07:38** these people to

**07:40** agree but I asked the executives if you

**07:44** take two under writers at random by how

**07:47** much do you expect them to vary in

**07:50** percentages of and and that question it

**07:54** turns out has an answer and without

**07:56** asking asking you I think we know what

**08:01** number is in your mind because we've

**08:03** we've asked that question of a large

**08:05** number of people and the number is

**08:07** roughly 10% people think that 10% is a

**08:11** reasonable difference between people we

**08:14** know that they won't agree perfectly

**08:16** because it's a matter of judgment and

**08:18** judgment is defined by the possibility

**08:21** of reasonable disagreement but we expect

**08:23** a limited amount of noise it turned

**08:26** out that the difference the the average

**08:31** difference between two Underwriters was

**08:34** 50% wow five times larger than the

**08:37** executives expected and that is really

**08:41** the reason for writing the book is that

**08:43** there was so much more noise Than People

**08:46** expected and this came as a complete

**08:49** surprise to their

**08:50** organization and Olivier soon joined and

**08:54** we worked together on on that

**08:57** topic and it turns out that it's not

**09:00** only in the insurance company but

**09:02** wherever you look at judgment you find

**09:05** noise and a lot of it and there's also a

**09:08** misunderstanding which explains why

**09:10** people have paid less attention to it

**09:11** the misunderstanding is the the

**09:13** misconception that it doesn't matter

**09:16** because on average it cancels out right

**09:19** noise is an error of average zero

**09:23** essentially and people tend to say well

**09:25** we care about bias because bias is the

**09:27** shared error but we don't care about

**09:29** noise because on average it cancels out

**09:31** of course that's wrong right if you

**09:33** think of the underwriters if on average

**09:35** they price correctly but sometimes they

**09:37** price way too low and sometimes it's way

**09:39** too high well when it's way too low the

**09:41** company is going to lose money by paying

**09:43** too much in claims and when it's way too

**09:44** high is going to lose money by losing

**09:47** business to a competitor so both

**09:49** mistakes are costly but when we think of

**09:52** Noise We tend to think that well average

**09:54** errors you on average errors cancel out

**09:56** they

**09:57** don't and that's what we want wanted to

**09:59** draw attention to so so it is about in a

**10:01** business context and we can come back to

**10:03** some more examples but in a business

**10:04** context there is money at stake

**10:06** essentially is what you're saying it's a

**10:08** financial cost oh absolutely it's a Mis

**10:11** it's an error what I actually the a

**10:14** notion I I should have mentioned earlier

**10:17** from the theory of measurement is that

**10:20** in the theory of measurement noise and

**10:23** bias are equally important in fact there

**10:26** is a Formula the ma the mathematics

**10:29** don't matter but the formula is that a

**10:32** measure of global inaccuracy is the

**10:35** square of the bias plus the square of

**10:38** the noise and that's just to give you an

**10:40** idea that bias and noise are independent

**10:43** sources of inaccuracy and in principle

**10:46** equally important in practice we think

**10:49** there is more noise than bias in

**10:52** judgment it has a cost as you point out

**10:55** but it's not just about the business

**10:56** cost another example which we looked at

**10:59** is Justice Criminal Justice and when you

**11:03** give the same cases this is an old study

**11:05** that was done in the US we're not aware

**11:07** of it being done anywhere else including

**11:09** here but I'm sure the results would be

**11:10** very similar if you give the same cases

**11:13** simplified cases vignettes to multiple

**11:18** experienced judges you will find a lot

**11:21** of variability in their judgments for

**11:23** some of those cases you will find some

**11:25** judges saying one you're in prison and

**11:27** others saying like in prison for some of

**11:30** these cases you will find some judges

**11:32** saying 15 years in prison and others

**11:34** saying no prison at all on average for a

**11:37** sentence of seven years you have almost

**11:39** four years of difference between two

**11:42** judges again looking at the same case

**11:44** we're not talking about individualized

**11:46** sentencing you taking care of the

**11:48** specifics of each situation that is

**11:50** already in the case we're talking about

**11:52** different judges looking at the same

**11:54** case this is not a business cost but

**11:57** this is a huge problem of fairness

**11:59** and one of the big consequences of noise

**12:02** is unfairness the thing that I found

**12:05** fascinating about that particular

**12:07** example was that it had been recognized

**12:09** in a proper study mandatory guidelines

**12:12** were put in place and then they were

**12:14** taken they there was so much the judges

**12:16** were so unhappy that they became

**12:18** advisory it's it's slightly more

**12:20** complicated in fact it was a long battle

**12:23** led by a judge called Franco in the US

**12:26** and then by Ted Kennedy when he was

**12:28** Senator which led to establishing

**12:31** guidelines that Define rather precisely

**12:35** what sentencing should be with some

**12:36** leeway to adjust to the situation and

**12:39** the possibility of going out of the

**12:40** guidelines if you Justified it Etc and

**12:42** then for technical reasons that have

**12:45** nothing to do with what we're discussing

**12:47** here the Supreme Court turned those

**12:50** guidelines into advisory guidelines and

**12:53** noise crept back in it's not because the

**12:55** judges were unhappy about them but the

**12:57** judges were unhappy but but they were

**12:59** unhappy and as soon as they could

**13:01** actually regain their discretion they

**13:05** used it to the full extent that they

**13:07** could

**13:09** discussing noise with judges is quite an

**13:12** interesting exercise because the you

**13:16** know the whole idea is there is this is

**13:19** a justice system Justice is being

**13:22** determined as being me of to defendants

**13:25** and the idea that there is noise in that

**13:27** system is acceptable to to the extent

**13:31** that I was actually invited to talk to a

**13:35** large group of Judges but what they what

**13:38** the organizers asked me to talk about

**13:40** was noise in

**13:42** medicine so uh noise drilling noise in

**13:46** Justice is quite quite complex but

**13:49** actually noise in medicine sorry carry

**13:51** on but what's the noise in medicine

**13:53** there is a lot of noise in medicine uh

**13:56** about as much noise as as wherever there

**13:59** is Judgment there is noise and so this

**14:02** is true for diagnosis this is true for

**14:05** treatment uh in in some cases it really

**14:08** is quite shocking we had uh a set of

**14:12** data sent to us from Harvard Medical

**14:15** School on the diagnosis of epilepsy from

**14:18** EG recordings and the average

**14:22** correlation between two physicians in

**14:25** judging cases is not exactly zero but

**14:28** not much higher than zero and in

**14:31** negative I think I if I recall correctly

**14:33** I think it was a small negative number

**14:35** no wasn't it I I don't think I remember

**14:37** it as slightly positive but Cas in

**14:40** either case it was very

**14:43** small adding adding to that view that

**14:45** medicine is an art rather than a science

**14:47** but so far I mean these are decisions

**14:49** that are made repeatedly so if you go

**14:50** back to the judge or the the doctor

**14:52** indeed that they're making similar

**14:54** decisions over and over again does it

**14:56** apply to oneoff decisions can you talk

**14:58** talk about noise in in singular

**15:01** decisions well we we ask ourselves that

**15:03** question because obviously when we

**15:05** measure a noise we me we when we Define

**15:07** no we Define it as variability between

**15:09** multiple decisions that should be

**15:12** identical that definition does not apply

**15:14** to a decision that you make only once

**15:16** but when we look at the things that

**15:19** cause noise which we haven't talked

**15:20** about yet the the psychological

**15:22** mechanisms behind noise it's very clear

**15:25** that they're also present when you make

**15:27** one of those one of life or death

**15:30** momentus change the world decisions you

**15:32** know brexit or no brexit is there noise

**15:35** in that decision well if you think that

**15:38** it's a judgment that is being made by a

**15:40** human being there must be because the

**15:42** same causes that create noise out there

**15:46** we we came to think of those singular

**15:49** decisions those unique decisions as

**15:52** repeated decisions that happen only once

**15:55** right if they were repeated you would

**15:56** actually see that there is noise in them

**15:58** you can't see it but that doesn't mean

**16:00** there isn't noise so so let's talk about

**16:03** the causes of noise I mean why do three

**16:06** judges facing very similar cases make

**16:09** three different decisions or three

**16:10** doctors looking at the same x-ray come

**16:13** to three different conclusions well

**16:14** there are several sources of noise

**16:16** actually there are three main sources of

**16:18** noise and one source of noise is

**16:23** differences consistent differences

**16:25** between noise in between judges in the

**16:28** of the judgments they make so some

**16:31** judges will be more severe than others

**16:33** so on average their sentences the

**16:36** sentences of one judge will be higher

**16:38** than the other this is well known this

**16:40** is quite recognized there is another

**16:42** source of noise which is also we is not

**16:46** surprising to people and this is noised

**16:49** within a judge that is the same judge uh

**16:53** in a good mood or in a bad mood it turns

**16:56** out on a sunny day or an rainy day and a

**16:59** hot day or a cooler day will pass make

**17:03** somewhat different judgments we call

**17:05** that occasion noise but then there is a

**17:09** third source of noise which turns out to

**17:12** be the most important and and I would

**17:16** call that it's a difference in the

**17:20** almost in personality if you

**17:22** will that and to see it in the decisions

**17:27** in the Justice context

**17:28** if you take sever if you take two

**17:31** judges and you show them 20 cases the

**17:35** new type of noise is that they will not

**17:37** order the cases in terms of severity in

**17:40** the same way so you might have one judge

**17:44** who is particularly shocked by uh when

**17:48** you know the defendants are uh young or

**17:51** when the victims are old and and there

**17:54** are differences in taste you can have a

**17:56** judge who is strongly affected by

**17:57** someone was similar to to a member of

**18:00** their family and that is that would be

**18:02** consistent and we call that pattern

**18:05** noise and it turns out that pattern

**18:08** noise is the more important the more

**18:11** interesting and the most difficult kind

**18:13** type of noise so pattern noise is not

**18:16** quite like group think is it where a

**18:18** group works but does that affect noise

**18:20** as well

**18:21** then group well let's clarify patter

**18:25** noise first okay go clarify P noise yes

**18:28** so

**18:29** p and that touches on something that you

**18:32** mentioned

**18:33** earlier that when people look at the

**18:36** world or at any problem in the world uh

**18:40** you have the sense every one of us had

**18:43** the sense that we see the world the way

**18:45** we do because that's the way it is so we

**18:49** have the sense that we're in touch with

**18:51** reality and if the world is as we see it

**18:56** we expect other reasonable people to see

**18:59** it in the same way and it turns out that

**19:02** they don't to an extent that is

**19:05** radically surprising so what what we

**19:08** have to take on board here is is the

**19:11** idea that people are really more

**19:13** different from each other in how they

**19:15** see any problem in the world than any of

**19:18** them would expect and that's the basic

**19:21** phenomenon of no and we also have to

**19:23** come to grips with the idea that that's

**19:26** not good news

**19:30** when when we say oh we're all different

**19:33** we we're all unique we're all diverse we

**19:36** have different views on everything we

**19:38** usually celebrate that we say that's

**19:40** beautiful that's diversity that's where

**19:41** creativity comes from that's where

**19:43** Innovation comes from but the thing is

**19:45** when you go to one doctor and he says

**19:47** you have this and then you go to another

**19:49** doctor and he says you have that you

**19:50** don't say oh beautiful

**19:52** creativity Innovation diversity no you

**19:55** say one of you guys must be wrong

**19:56** perhaps both by the way so when we think

**19:59** there is a correct answer which is how

**20:01** we Define judgment that kind of

**20:04** diversity is what we call pattern noise

**20:06** the fact that people project their own

**20:08** history and their own sensibilities and

**20:10** their own biases on a situation is what

**20:14** creates this pattern noise and it's not

**20:16** good

**20:17** news isn't that what people call human

**20:20** agency it is what people call projecting

**20:23** their personality into their judgment

**20:25** and of course if you expect them to do

**20:28** that that's beautiful if you are

**20:30** choosing whom to marry in that way

**20:33** congratulations that's how I would

**20:35** strongly recommend you do it if you're

**20:37** making a hiring decision on behalf of

**20:39** the BBC we might have a

**20:42** problem so to go back to the the earlier

**20:45** point then if you have a meeting and the

**20:47** first person stands up and says I

**20:50** definitely want to do it this way uh you

**20:52** know I definitely think we should only

**20:54** employ people who wear red t-shirts and

**20:56** the second person gets up and says or

**20:59** are they more likely to to make the same

**21:01** decision how do people affect one

**21:03** another how does that create noise

**21:05** groups tend to create noise in just the

**21:09** way that you said in that when you want

**21:13** to minimize noise or to reduce noise

**21:16** then you want the judgments of people to

**21:19** be independent of each other so you

**21:22** would want witnesses to the same crime

**21:24** to discuss their testimony before they

**21:27** give it and this is precisely what

**21:30** happens in a meeting in a meeting people

**21:32** influence each other and the first

**21:35** person to speak has a disproportionate

**21:37** influence on the others and that is an

**21:39** element of noise because the first

**21:41** person to speak is not necessarily the

**21:44** best is not necessarily the most

**21:47** accurate uh so so are you saying in a

**21:50** sense that we're disinclined to

**21:53** disagree well certainly there is a

**21:56** massive amount of conformity that goes

**21:58** on and this is where you know otherwise

**22:01** meetings might go on much longer but uh

**22:05** and and as it is they G too long but

**22:08** there is there is Convergence and there

**22:11** is in in that sense more convergence

**22:15** than we would want that is ideally the

**22:17** ideal form for a meeting would be one in

**22:21** which people to which people come

**22:23** prepared each with their own

**22:26** opinion and

**22:28** and you establish the amount of

**22:31** Divergence in their opinions and now you

**22:33** discuss it and now you try to reach

**22:37** convergence but the automatic kind of

**22:39** convergence that happens in a debate is

**22:42** actually not a noise reduction

**22:45** mechanism when we were working on this

**22:47** topic we interviewed an organizational

**22:48** psychologist who worked with the

**22:50** admissions officers of

**22:52** universities and he told us the

**22:54** following story he had come to an

**22:56** admissions office in large University

**22:58** where admissions officers would read the

**23:01** application essays of applicants and

**23:03** they would grade them and then they

**23:04** would give the essay with the grade to

**23:06** another admissions officer who would

**23:08** make a separate judgment so our friend

**23:11** recommended of course that they should

**23:14** make those judgments Independent by

**23:15** hiding the grade given by the first

**23:18** officer from the Second Officer so that

**23:20** these two judgments could be different

**23:22** and the answer he got was oh that's how

**23:24** we used to do it but we disagreed so

**23:26** much that we adopted the current method

**23:30** and this is a perfect summary of the

**23:32** trade-off that organizations make

**23:35** between consensus which they must

**23:37** achieve at some point and disagreement

**23:41** which they do have they tend to suppress

**23:44** dis agreement as best they can in order

**23:46** to achieve consensus and to be able to

**23:47** make decisions and what we're saying

**23:49** here is if you want consensus to be

**23:52** achieved on something closer to the best

**23:54** possible answer you might want to delay

**23:57** the moment when you you achieve that

**23:58** consensus and to make sure that you get

**24:00** independent opinions at the beginning of

**24:01** the process it's interesting that we're

**24:03** talking about convergence and people

**24:05** wanting to agree when certainly in

**24:08** politics what we talk about all the time

**24:10** is

**24:11** polarization well

**24:15** uh we talk about judgments and and we

**24:20** try to avoid judgments of values and

**24:22** differences in values so the assumption

**24:24** is when when you have underr or or

**24:29** judges they speak for an

**24:32** organization the organization speaks

**24:34** through individual

**24:36** functionaries and you want an

**24:38** organization to speak in one voice uh

**24:41** and and noise is a failure is a

**24:44** cacophony but when it comes to values or

**24:47** when it comes to politics in a democracy

**24:50** you

**24:50** certainly want to allow differences and

**24:53** Divergence of values and and and you

**24:56** certainly don't want to impose noise

**24:59** reduction oh that's interesting no noise

**25:01** reduction in politics H um so let's just

**25:04** think about some of the things that this

**25:06** has thrown up that convergence in a

**25:09** sense hide well it's a way of avoiding

**25:11** the fact that there is noise why do you

**25:14** think institutions like educational

**25:17** establishments or businesses are

**25:20** reluctant to confront the idea of noise

**25:23** Beyond just wanting to get on and make

**25:25** any decision whether it's good or bad

**25:28** well I think first they are not aware of

**25:30** the amount of noise that there is I

**25:32** think you know every time we show the

**25:35** results of a noise audit to an

**25:36** organization or we just share the

**25:38** results of noise audits performed in

**25:39** other organizations people are very

**25:41** surprised we could hear the surprise in

**25:43** the room when Denny mentioned the 50%

**25:45** number from the insurance company this

**25:47** is very surprising so I think the

**25:50** obvious answer I mean the the first and

**25:52** most important answer is they just don't

**25:54** know and that's partly why we wrote the

**25:57** book really

**25:58** but in lots of fields you're getting the

**26:01** use of algorithms to I think about

**26:03** Actuarial work which is obviously in the

**26:05** insurance field you know humans used to

**26:07** do all the risk calculations and now a

**26:10** proportion will be done by a computer

**26:13** but they still as far as I can tell ask

**26:16** humans to make the final judgment is

**26:18** that because we don't want to hand over

**26:20** that that power uh it clearly depends on

**26:23** the topic I mean there is a long history

**26:25** about 70 years of comparing the

**26:28** judgments of people to formulas and and

**26:31** more recently to algorithms that are

**26:34** generated by Ai and and humans do not

**26:37** fair very well in those comparisons and

**26:40** the main reason for human inferiority is

**26:44** actually noise because algorithms have

**26:47** and rules simple or complex have the

**26:50** advantage of being Noise free and that

**26:53** gives them a real leg up when it comes

**26:56** to accuracy

**26:58** but but isn't the obvious Counterpoint

**27:00** to that that algorithms can make matters

**27:02** worse because they will have inherent in

**27:04** them the biases of of whoever wrote the

**27:07** code they

**27:09** will although there is no reason why

**27:13** those biases should be worse than the

**27:15** biases of the humans again it depends

**27:17** what the comparison point is you see the

**27:20** bias of algorithms is a big topic right

**27:23** it's being discussed everywhere and it

**27:24** should be because it's real but the B of

**27:27** algorithms are there because algorithms

**27:29** are trained on data that is the past

**27:33** decisions and the past judgments of

**27:35** humans so what algorithms are doing is

**27:37** they are the mirror of our own biases

**27:40** they are not worse what makes the bias

**27:44** more visible when you actually have an

**27:46** algorithm is two things first you can

**27:47** actually run a million decisions into

**27:49** the algorithms and see how many you know

**27:53** biased decisions there are which you

**27:54** cannot do with a human being second the

**27:57** algorith is Noise free as Denny pointed

**27:59** out it's very consistent in its biases

**28:02** so if you train your hiring algorithm

**28:05** and you tell the algorithm you these are

**28:07** all the people who have been promoted in

**28:09** my company figure out what it takes to

**28:11** be successful in my company and the

**28:13** algorithm comes back and says you have

**28:15** to hire men not women is the algorithm

**28:18** sexist no you

**28:20** are the reason you hadn't noticed is

**28:23** because you are noisy in your sexism

**28:25** even the even the most sexist recruiter

**28:28** is occasionally going to hire a woman

**28:30** the algorithm won't do that the

**28:32** algorithm will say well it's pretty

**28:33** clear that here to be successful you

**28:35** need to be a man so the the absence of

**28:37** noise makes the bias more visible it

**28:40** doesn't actually make it worse it also

**28:42** makes it easier to solve if you want to

**28:45** solve it you can actually design an

**28:47** algorithm that would be devoid of such

**28:49** biases so algorithms could make very

**28:53** good decisions do we want them to that's

**28:56** a very different

**28:58** issue yeah when you know algorithms are

**29:03** biased biased algorithms are poorly

**29:06** constructed algorithms and typically the

**29:09** bias enters somewhere in the definition

**29:13** of what it is that you're using as as a

**29:17** Criterion for Success so for

**29:20** example Amazon uh tried to develop an

**29:23** algorithm to replicate its high

**29:27** decisions and then that algorithm turned

**29:30** out the way they constructed it they

**29:33** looked at people who had been hired or

**29:35** had been rejected and they looked for

**29:38** the what distinguished them and then

**29:41** they found there was a gender bias

**29:44** because people had in fact preferred men

**29:47** to women in their hiring and that went

**29:51** on to the algorithm that's a flawed

**29:54** Criterion that is when you want to hire

**29:57** hire properly you should not go by the

**29:59** previous hiring decisions then you are

**30:02** guaranteed to replicate the bias but

**30:05** when you do it properly and then bias is

**30:09** not a necessary feature of uh algorithm

**30:13** although as Olivier pointed out the

**30:16** biases of algorithms are always going to

**30:19** be more visible because they're going to

**30:21** be more detectable because they're not

**30:24** masked by noise whereas the biases of

**30:26** humans are very frequently M away

**30:30** noise I'm beginning to think that you

**30:32** think expertise is

**30:35** overrated is it a concept we shouldn't

**30:37** even talk

**30:42** about expertise in the

**30:45** field if you want us to say oh we've had

**30:47** enough of experts we're not going to say

**30:50** that

**30:54** no there is an interesting question when

**30:57** you think about expertise how we

**30:59** evaluate

**31:01** expertise and this is that many of the

**31:04** judgments where we look at experts are

**31:08** actually

**31:10** unverifiable that is there are judgments

**31:12** that we make like long-term

**31:15** forecasts uh which cannot be verified

**31:17** and yet so you would think that people

**31:20** who specialize in that kind of judgments

**31:23** how how do we know how to

**31:25** distinguish a

**31:28** good professional from a weaker

**31:31** professional and there it turns out that

**31:35** there are ways for people to emerge as

**31:39** experts we call them respect experts

**31:42** because they're experts because they're

**31:44** respected not because of the quality of

**31:46** their performance because their

**31:48** performance cannot be evaluated so there

**31:50** were expert astrologers and that is very

**31:54** useful to remember that some astrologers

**31:57** ERS had the respect of their peers and

**31:59** they were taken more seriously than

**32:01** others and and they operated as experts

**32:05** and what

**32:06** creates a respect expert is

**32:10** self-confidence eloquence intelligence I

**32:13** mean a lot of properties that we want

**32:15** but they do not guarantee good

**32:17** performance good performance requires

**32:20** some verifiable Criterion some way some

**32:24** feedback about the accuracy of the drug

**32:27** made now if you are a chess player if

**32:30** you are a weather forecaster if you are

**32:33** an investor your expertise can be

**32:36** measured can be assessed against

**32:38** objective benchmarks and we can decide

**32:40** after a while that you are an excellent

**32:42** or a poor weather forecaster because we

**32:44** see how accurate you are these are not

**32:47** the respect experts they are

**32:49** the experts you can see the outcomes yes

**32:53** so a fair chunk of the book is actually

**32:56** a sort of a how-to manual and by that I

**32:58** mean you sort of say how to detect and

**33:01** reduce

**33:02** noise give us some pointers what's what

**33:04** if I if I decide this is a really bad

**33:07** thing I want my decisions to be more

**33:08** efficient to be more accurate what do I

**33:10** do so we've talked about algorithms and

**33:13** the basic idea of algorithms is to say

**33:16** as Denny pointed out wherever there is

**33:17** human judgment there is going to be

**33:19** noise if you want there not to be noise

**33:21** take out the human judgment let's put

**33:23** that aside because there's many

**33:25** situations where it's either impractical

**33:28** or undesirable for either good or bad

**33:31** reasons to use algorithms and you will

**33:33** want to use human judgment when you are

**33:35** going to use human judgment the approach

**33:38** or the set of approaches we suggest to

**33:41** reduce noise in human judgment is what

**33:43** we call decision hygiene and the reason

**33:46** we use this odd phrase is because the

**33:50** analogy of washing your hands is apt

**33:53** here we when we wash our hands we are

**33:55** not saying oh this was this germ that I

**33:57** take out and this is this disease that

**33:59** I've avoided we don't know what problem

**34:01** we've avoided we just know it's good

**34:03** prevention we're putting the process

**34:05** under control and decision hygiene does

**34:08** that it puts the decision process it

**34:10** puts the process of judgment under some

**34:12** sort of control how do you do that we've

**34:14** talked about aggregating independent

**34:16** opinions that's a good way to introduce

**34:18** decision hygiene making sure these

**34:20** opinions are independent if you

**34:22** aggregate opinions through discussion as

**34:25** we pointed out that actually does more

**34:27** harm than good another way would be to

**34:30** structure your decisions yet another way

**34:32** would be to use relative judgments

**34:35** rather than absolute judgments wherever

**34:36** you can there's a whole series of things

**34:39** which taken together help introduce a

**34:42** little bit more discipline into your

**34:44** decision

**34:45** making and I should point out here that

**34:49** when we talk about noise reduction or

**34:51** about decision hygiene we had in writing

**34:54** the book we had organizations in mind

**34:57** that the book is not self-help for

**35:00** individuals um I'm quite skeptical about

**35:03** the ability of individuals to improve

**35:06** their own thinking but organizations

**35:08** have procedures they have processes it

**35:12** is possible although it this is not

**35:14** typical that there's few organizations

**35:17** that I know about have designed

**35:20** processes for reaching judgments or

**35:22** decisions but we believe that more

**35:25** organizations should have designs for

**35:29** making decisions and design that

**35:31** embodied or that incorporate the

**35:33** principles of decision hygiene so

**35:36** organizations have an opportunity that

**35:39** individuals do not

**35:40** have at the beginning you you described

**35:44** the the underwriters at the insurance

**35:46** agency understand realizing how much

**35:48** noise there was but is it okay so there

**35:51** you identified it but is it an expensive

**35:54** process a difficult process for for for

**35:56** organizations to undertake do you think

**35:58** there will be a reluctance to identify

**36:01** noise well certainly you know in part

**36:04** because noise is so abstract that as you

**36:08** can point out to an error to a single

**36:10** error and it looks like a bias and it's

**36:13** easy to explain as Olivier was

**36:15** mentioning earlier noise there is no

**36:18** single error unless it's a complete

**36:20** outlier but normally you cannot point to

**36:23** an eror and say this is noise it takes

**36:26** more than error it takes the variability

**36:28** of errors this gives noise that abstract

**36:31** sort of character that makes it quite

**36:33** difficult to conceive of and it is true

**36:38** that when you try to mitigate noise in

**36:41** an organization you will certainly face

**36:43** resistance and you will face resistance

**36:45** which in part is fully Justified because

**36:48** there is the danger of bureaucracy there

**36:52** is a danger of imposing procedures that

**36:55** turn mechanical and that make people

**36:58** less involved in the judgments that they

**37:00** make so there is really a tradeoff

**37:03** between design decisions and spontaneous

**37:06** decisions and it's up to individual

**37:09** organizations to find their way in that

**37:11** tradeoff and before you even get to

**37:14** noise reduction just becoming aware of

**37:16** noise is something that some

**37:17** organizations that many organizations

**37:19** will resist because noise is

**37:22** embarrassing you we've talked about one

**37:24** problem with noise which is that is

**37:25** costly we've talked about not another

**37:27** which is that it creates unfairness

**37:29** there's a third reason why noise is

**37:32** detrimental which is that it's

**37:34** embarrassing to The credibility of the

**37:36** organization if in an insurance company

**37:39** you become aware that you might get a

**37:41** quote for $100,000 and the next customer

**37:45** with exactly the same need might get a

**37:46** quote for2

**37:48** 200,000 that doesn't reflect very well

**37:50** on the organization and it's very

**37:52** tempting to move on and talk about

**37:55** something else and I can tell you that

**37:57** this is what happened in the insurance

**38:00** company they they hadn't known about the

**38:03** problem and once they were made aware of

**38:06** the problem I think they forgot it very

**38:09** quickly but but does that perhaps

**38:11** reflect that the to come back to this

**38:13** idea of human agency that the power to

**38:16** exercise discretion it may be far less

**38:19** efficient but actually within a company

**38:22** within a hierarchy it's a necessary

**38:24** trade-off I'm the top dog I know what

**38:26** I'm doing and I'm going to tell

**38:28** you well it's it's certain very tempting

**38:32** I we we had an interesting conversation

**38:34** yesterday with a very senior HR

**38:36** executive who said you know we read

**38:38** everything that you talk about and it's

**38:41** clear that it would improve the odds of

**38:43** hiring the best people but as you point

**38:45** out yourself hygiene is a bit tedious

**38:48** it's much more fun to hire people I

**38:52** like and if that's what you're going to

**38:54** do that's fine it's your business if I

**38:57** were your boss I would probably raise

**39:00** some questions but I'm not so you do

**39:03** whatever and there is another reason

**39:05** that really comes up and that judgment

**39:07** is typically and quite often very highly

**39:10** fallible so for example in

**39:14** hiring real accuracy in hiring is

**39:17** impossible it's impossible because

**39:20** future performance is not fully

**39:22** predictable a lot of things are going to

**39:24** happen on the job that can be known in

**39:27** advance that are not characteristics of

**39:29** the individual and that will determine

**39:31** the individual's performance so the best

**39:34** that you can do in hiring is quite poor

**39:38** but you can do worse than that and that

**39:40** is and and people do but what you get as

**39:44** a response quite often well if all you

**39:46** can promise me is that performance will

**39:49** be quite poor I might as well trust my

**39:51** God and that's a

**39:55** mistake
