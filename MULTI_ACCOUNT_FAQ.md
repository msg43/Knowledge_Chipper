# Multi-Account Download Strategy - FAQ

## Q1: How do I import playlists into throwaway accounts?

**Short Answer**: You don't! ‚úÖ

### How It Actually Works

Throwaway accounts are **only used for authentication** (cookies), NOT for accessing content.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ YOUR MAIN ACCOUNT                                       ‚îÇ
‚îÇ  - Has playlists you want to download                  ‚îÇ
‚îÇ  - Export playlist URLs (yt-dlp, browser extension)    ‚îÇ
‚îÇ  - You never log in with this account for downloading  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
              Export URLs (list of 7000 videos)
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ THROWAWAY ACCOUNTS (3-5 accounts)                      ‚îÇ
‚îÇ  - Only provide authentication cookies                 ‚îÇ
‚îÇ  - Don't need any playlists/subscriptions              ‚îÇ
‚îÇ  - Just need to be logged in                           ‚îÇ
‚îÇ  - Never "own" the content being downloaded            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
              Download videos using throwaway cookies
```

### Example Workflow

```python
# Step 1: Get URLs from YOUR main account
my_playlists = [
    "https://youtube.com/playlist?list=PLxxx...",
    "https://youtube.com/playlist?list=PLyyy...",
]

# Export to list of individual video URLs
urls = export_playlist_urls(my_playlists)  
# Result: 7000 video URLs

# Step 2: Download using throwaway account cookies
# The cookies authenticate the request, but don't need to "own" the videos

scheduler = MultiAccountDownloadScheduler(
    cookie_files=[
        "throwaway_account_1_cookies.txt",  # Just for auth
        "throwaway_account_2_cookies.txt",
        "throwaway_account_3_cookies.txt",
    ]
)

# This downloads videos from YOUR playlists
# Using throwaway accounts' authentication
results = await scheduler.download_batch_with_rotation(urls)
```

### Setup Steps

**1. Create Throwaway Gmail Accounts** (15 min)

```
Account 1: throwaway.yt.downloads.1@gmail.com
Account 2: throwaway.yt.downloads.2@gmail.com
Account 3: throwaway.yt.downloads.3@gmail.com

Settings:
- Use fake names, burner emails
- Mark as 18+ during setup
- No phone verification needed for downloading
```

**2. Make Accounts Look "Normal"** (optional, 30 min)

```
For each throwaway account:
1. Log in to YouTube
2. Watch 2-3 random videos (makes activity look real)
3. Subscribe to 1-2 popular channels (optional)
4. Let account sit for 24 hours before using (optional)
```

**3. Export Cookies** (5 min per account)

```
For each throwaway account:
1. Install browser extension "Get cookies.txt" (Chrome/Firefox)
2. Visit youtube.com while logged in
3. Click extension ‚Üí Export cookies
4. Save as: cookies_account_1.txt, etc.
5. Place in project directory
```

**4. Get Your Video URLs** (from YOUR main account)

```bash
# Option A: Use yt-dlp to export playlist URLs
yt-dlp --flat-playlist --print url "https://youtube.com/playlist?list=PLxxx" > urls.txt

# Option B: Use browser extension
# "YouTube Playlist Helper" or similar

# Option C: Manual export
# Use YouTube Data API to get playlist items
```

**5. Run Downloads**

```python
# Load your URLs (from YOUR playlists)
with open("urls.txt") as f:
    my_urls = [line.strip() for line in f]

# Download using throwaway account authentication
scheduler = MultiAccountDownloadScheduler(
    cookie_files=[
        "cookies_account_1.txt",
        "cookies_account_2.txt", 
        "cookies_account_3.txt",
    ]
)

results = await scheduler.download_batch_with_rotation(my_urls)
```

### Why This Works

YouTube cookies provide:
- ‚úÖ **Authentication** (you're a logged-in user, not anonymous bot)
- ‚úÖ **Age-gated content access** (if account is 18+)
- ‚úÖ **Geographic access** (based on account location)
- ‚úÖ **Better rate limits** (authenticated users treated better)

But they **don't** require:
- ‚ùå Account to own/subscribe to the content
- ‚ùå Account to have playlists saved
- ‚ùå Any relationship between account and videos

**The throwaway accounts are just authentication tokens, nothing more!**

---

## Q2: If there are URL overlaps, will the app skip duplicates?

**Short Answer**: Yes! ‚úÖ The system has comprehensive duplicate detection.

### How Duplicate Detection Works

**Key Insight**: Deduplication is based on **video_id**, not URL or account.

```python
# All these URLs point to the SAME video (video_id: "dQw4w9WgXcQ")
"https://www.youtube.com/watch?v=dQw4w9WgXcQ"
"https://youtu.be/dQw4w9WgXcQ"
"https://www.youtube.com/embed/dQw4w9WgXcQ"
"https://www.youtube.com/watch?v=dQw4w9WgXcQ&list=PLxxx"  # From playlist

# System extracts "dQw4w9WgXcQ" from ALL of these
# Database lookup is by video_id (not URL)
# Once downloaded by ANY account, ALL accounts skip it ‚úÖ
```

### Multi-Account Scenario

```
Time: 10:00 AM
Account 1 processes URL batch 1-2500:
  ‚îú‚îÄ URL #1:  https://youtube.com/watch?v=abc123
  ‚îÇ   ‚îú‚îÄ Extract video_id: "abc123"
  ‚îÇ   ‚îú‚îÄ Check database: NOT FOUND
  ‚îÇ   ‚îú‚îÄ Download: SUCCESS ‚úÖ
  ‚îÇ   ‚îî‚îÄ Save to database with video_id="abc123"
  ‚îÇ
  ‚îú‚îÄ URL #15: https://youtube.com/watch?v=def456
  ‚îÇ   ‚îî‚îÄ Download: SUCCESS ‚úÖ
  ‚îÇ
  ‚îî‚îÄ URL #2500: ...

Time: 10:30 AM
Account 2 processes URL batch 2501-5000:
  ‚îú‚îÄ URL #2501: https://youtube.com/watch?v=abc123  (DUPLICATE!)
  ‚îÇ   ‚îú‚îÄ Extract video_id: "abc123"
  ‚îÇ   ‚îú‚îÄ Check database: FOUND (Account 1 downloaded it at 10:00 AM)
  ‚îÇ   ‚îú‚îÄ Result: SKIP ‚úÖ
  ‚îÇ   ‚îî‚îÄ Log: "Skipping duplicate video abc123: Already processed"
  ‚îÇ
  ‚îú‚îÄ URL #2502: https://youtu.be/def456  (Different URL format, SAME video!)
  ‚îÇ   ‚îú‚îÄ Extract video_id: "def456"
  ‚îÇ   ‚îú‚îÄ Check database: FOUND (Account 1 downloaded it)
  ‚îÇ   ‚îú‚îÄ Result: SKIP ‚úÖ
  ‚îÇ   ‚îî‚îÄ Log: "Skipping duplicate video def456: Already processed"
  ‚îÇ
  ‚îî‚îÄ URL #5000: ...

Time: 11:00 AM
Account 3 processes URL batch 5001-7500:
  ‚îî‚îÄ Same deduplication logic
  ‚îî‚îÄ Skips all duplicates from Account 1 and Account 2 ‚úÖ
```

### Database Schema

The deduplication uses SQLite `media_sources` table:

```sql
CREATE TABLE media_sources (
    media_id TEXT PRIMARY KEY,  -- The video_id (e.g., "abc123")
    url TEXT,                   -- Original URL (for reference)
    title TEXT,
    audio_downloaded BOOLEAN,
    metadata_complete BOOLEAN,
    processed_at TIMESTAMP,
    ...
);

-- Example entries after Account 1 downloads:
INSERT INTO media_sources VALUES (
    'abc123',  -- video_id (PRIMARY KEY - prevents duplicates)
    'https://youtube.com/watch?v=abc123',
    'Example Video Title',
    TRUE,
    TRUE,
    '2025-10-27 10:05:23',
    ...
);

-- When Account 2 tries the same video:
SELECT * FROM media_sources WHERE media_id = 'abc123';
-- FOUND ‚Üí Skip download
```

### Code Implementation

The deduplication service is already integrated:

```python
# File: src/knowledge_system/utils/deduplication.py
class VideoDeduplicationService:
    def check_duplicate(self, url: str) -> DeduplicationResult:
        """Check if video already exists in database"""
        
        # 1. Extract video_id from URL
        video_id = self.extract_video_id(url)
        # "https://youtube.com/watch?v=abc123" ‚Üí "abc123"
        
        # 2. Check database
        existing_video = self.db.get_video(video_id)
        
        # 3. Return result
        if existing_video:
            return DeduplicationResult(
                video_id=video_id,
                is_duplicate=True,
                skip_reason=f"Already processed on {existing_video.processed_at}"
            )
        else:
            return DeduplicationResult(
                video_id=video_id,
                is_duplicate=False
            )
```

### Multi-Account Integration

```python
class MultiAccountDownloadScheduler:
    def __init__(self, cookie_files, ...):
        # Single shared deduplication service for ALL accounts
        self.dedup_service = VideoDeduplicationService()
    
    async def download_batch_with_rotation(self, urls):
        # Step 1: Check ALL URLs for duplicates BEFORE downloading
        unique_urls, duplicate_results = self.dedup_service.check_batch_duplicates(
            urls,
            DuplicationPolicy.SKIP_ALL
        )
        
        # Log results
        logger.info(
            f"Deduplication complete: "
            f"{len(unique_urls)} unique, "
            f"{len(duplicate_results)} duplicates"
        )
        
        # Step 2: Download ONLY unique URLs (distributed across accounts)
        for url in unique_urls:
            account = await self.get_available_account()
            result = await account.download_single(url)
            
            if result["success"]:
                # Video is now in database
                # Other accounts will skip it if encountered
                pass
```

### Example Output

```
$ python process_batch.py --urls urls.txt --accounts 3

üîç Checking 7,000 URLs for duplicates...
‚úÖ Deduplication complete: 4,237 unique, 2,763 duplicates skipped

Breakdown:
  - Total URLs provided: 7,000
  - Unique videos: 4,237 (60.5%)
  - Duplicates skipped: 2,763 (39.5%)
  
üí∞ Time saved by skipping duplicates:
  - Processing time saved: ~46 hours
  - Download bandwidth saved: ~165 GB

Starting downloads with 3 accounts...

[Account 1] Downloaded: video_001.m4a (1/4237)
[Account 2] Downloaded: video_002.m4a (2/4237)
[Account 3] Downloaded: video_003.m4a (3/4237)
[Account 1] Skipping duplicate: video_abc123 (already processed by Account 1 at 10:05:23)
[Account 2] Skipping duplicate: video_def456 (already processed by Account 2 at 10:06:15)
...

üìä Final statistics:
  Total URLs: 7,000
  Unique videos: 4,237
  Successfully downloaded: 4,235 (99.95%)
  Failed: 2 (0.05%)
  Duplicates skipped: 2,763
  Time saved: 46 hours
```

### What Gets Considered a Duplicate

**Duplicates** (will be skipped):
- ‚úÖ Same video_id, different URL format
- ‚úÖ Same video in multiple playlists
- ‚úÖ Already downloaded by ANY account
- ‚úÖ Already processed (transcribed + mined)

**NOT duplicates** (will be downloaded/processed):
- ‚ùå Different videos (different video_ids)
- ‚ùå Video exists but download failed (incomplete in database)
- ‚ùå Video exists with different processing settings (if policy allows)

### Deduplication Policies

```python
# Default (recommended)
DuplicationPolicy.SKIP_ALL  
# Skip everything already in database

# Alternative policies (configurable)
DuplicationPolicy.ALLOW_RETRANSCRIBE  
# Allow re-transcribing with different settings

DuplicationPolicy.ALLOW_RESUMMARY  
# Allow re-mining with different LLM models

DuplicationPolicy.FORCE_REPROCESS  
# Reprocess everything (ignore database)
```

---

## Summary

### Q1: Importing Playlists

**Answer**: Don't import playlists to throwaway accounts. Get URLs from YOUR main account's playlists, then download using throwaway account cookies for authentication.

**Setup**: 
1. Create 3 throwaway Gmail accounts (15 min)
2. Export cookies from each (5 min each)
3. Export URLs from YOUR playlists (5 min)
4. Run downloads (automated)

### Q2: Duplicate Detection

**Answer**: Yes, comprehensive duplicate detection is built-in and works across all accounts. 

**How**:
- Deduplication by video_id (not URL)
- Shared database across all accounts
- Automatic skipping with logging
- Typically saves 30-40% time on real playlists

**Expected Results**:
```
7,000 URLs ‚Üí ~4,200 unique videos (40% duplicates typical)
Time saved: ~46 hours
Bandwidth saved: ~165 GB
```

Both features work automatically - no special configuration needed! ‚úÖ
