# Knowledge System Configuration Template
# Copy this file to settings.yaml and customize for your needs

# Application Settings
app:
  name: "Knowledge System"
  version: "0.1.0"
  debug: false

# File Paths
paths:
  data_dir: "~/Documents/KnowledgeSystem"
  output_dir: "~/Documents/KnowledgeSystem/output"
  cache_dir: "~/Documents/KnowledgeSystem/cache"
  logs_dir: "./logs"

# Performance & Hardware Optimization
performance:
  # Performance profile selection
  # Options: auto, battery_saver, balanced, high_performance, maximum_performance
  profile: "auto"
  
  # Hardware detection settings
  enable_hardware_detection: true
  
  # Override settings (leave null to use profile defaults)
  override_whisper_model: null  # Options: tiny, base, small, medium, large, large-v3
  override_device: null         # Options: auto, cpu, cuda, mps
  override_batch_size: null     # Number (8-64)
  override_max_concurrent: null # Number (1-16)
  
  # Use case optimization
  # Options: general, large_batch, mobile
  optimize_for_use_case: "general"
  
  # Force specific hardware capabilities (advanced users only)
  force_mps: false      # Force MPS acceleration on Apple Silicon
  force_coreml: false   # Force CoreML acceleration on Apple Silicon

# Thread Management & Resource Control
thread_management:
  # OpenMP thread count for transcription processing
  # Default: min(8, cpu_count) - adjust based on your system
  omp_num_threads: 8
  
  # Maximum number of files to process simultaneously
  # Default: min(4, cpu_count // 2) - prevents resource contention
  max_concurrent_files: 4
  
  # Thread limit per individual process
  # Default: min(4, cpu_count // 2) - controls per-process resources
  per_process_thread_limit: 4
  
  # Enable/disable parallel processing of multiple files
  enable_parallel_processing: true
  
  # Force sequential processing to avoid all resource conflicts
  # Set to true if you experience truncation or memory issues
  sequential_processing: false
  
  # Enable tokenizer parallelism (may show warnings but can improve performance)
  tokenizers_parallelism: false
  
  # Enable MPS fallback for PyTorch on macOS
  pytorch_enable_mps_fallback: true

# Transcription Settings
transcription:
  whisper_model: "base"  # tiny, base, small, medium, large, large-v3
  use_gpu: true
  diarization: false
  min_words: 50
  use_whisper_cpp: false  # Use whisper.cpp with Core ML acceleration on macOS

# LLM Settings
llm:
  provider: "openai"  # openai, claude, local
  model: "gpt-4"
  max_tokens: 10000
  temperature: 0.1

# Local LLM Settings (for Ollama/LM Studio)
local_config:
  base_url: "http://localhost:11434"
  model: "qwen2.5:72b-instruct-q6_K"    # For large documents, consider: phi3:mini-128k (128K context)
  max_tokens: 10000                      # For 128K models, you can increase to 15000-20000
  temperature: 0.1

# API Keys (set these in your environment or directly here)
api_keys:
  openai_api_key: "${OPENAI_API_KEY}"
  anthropic_api_key: "${ANTHROPIC_API_KEY}"
  youtube_api_key: "${YOUTUBE_API_KEY}"
  webshare_username: "${WEBSHARE_USERNAME}"
  webshare_password: "${WEBSHARE_PASSWORD}"

# Processing Options
processing:
  batch_size: 10
  concurrent_jobs: 2
  retry_attempts: 3
  timeout_seconds: 300

# YouTube Processing Options
youtube_processing:
  # Automatically disable delays when rotating proxies are available
  disable_delays_with_proxy: false
  
  # Whether to use delays when proxies are configured
  use_proxy_delays: true
  
  # Rate limiting delays (in seconds)
  metadata_delay_min: 0.5
  metadata_delay_max: 2.0
  transcript_delay_min: 1.0
  transcript_delay_max: 3.0
  api_batch_delay_min: 1.0
  api_batch_delay_max: 3.0

# Maps of Content (MOC) Generation
moc:
  default_theme: "topical"  # topical, chronological, hierarchical
  default_depth: 2
  
  # Content extraction settings
  extract_people: true
  extract_tags: true
  extract_mental_models: true
  extract_jargon: true
  extract_beliefs: true
  
  # Minimum thresholds
  min_people_mentions: 2
  min_tag_occurrences: 3
  min_belief_confidence: 0.7

# Monitoring & Logging
monitoring:
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file_max_size: 10  # MB
  log_file_backup_count: 5
  
  # Performance monitoring
  enable_performance_tracking: true
  track_processing_times: true
  track_resource_usage: true

# Summarization Settings (inherits from llm by default)
summarization:
  provider: "openai"
  model: "gpt-4"
  max_tokens: 10000  # Increased for better summaries
  temperature: 0.1

# =============================================================================
# PERFORMANCE PROFILES EXPLANATION
# =============================================================================

# The Knowledge System automatically detects your hardware and optimizes
# performance based on your Mac's capabilities. Here's what each profile does:

# AUTO (Recommended):
# - Automatically selects the best profile for your hardware
# - M3 Ultra with 128GB RAM → Maximum Performance
# - M2 Pro with 32GB RAM → High Performance  
# - M1 with 16GB RAM → Balanced

# BATTERY_SAVER:
# - Minimal resource usage for laptop/mobile use
# - Sequential processing, single file at a time
# - Uses "tiny" Whisper model for speed
# - Best for: MacBook Air, preserving battery life

# BALANCED (Default):
# - Good balance of performance and resource usage
# - Moderate parallel processing
# - Uses "base" Whisper model
# - Best for: Most users, general-purpose work

# HIGH_PERFORMANCE:
# - Maximizes performance within thermal limits
# - High parallel processing
# - Uses recommended model for your hardware
# - Best for: Mac Studio, iMac Pro, heavy workloads

# MAXIMUM_PERFORMANCE:
# - Pushes hardware to absolute limits
# - Maximum parallel processing and batch sizes
# - Uses largest supported Whisper model
# - Prioritizes MPS over CoreML for large batches
# - Best for: M3 Ultra, M2 Ultra, professional use

# =============================================================================
# HARDWARE-SPECIFIC OPTIMIZATIONS
# =============================================================================

# The system automatically detects and optimizes for:
# - Chip type (M1, M2, M3 variants)
# - CPU cores (8-core to 24-core)
# - GPU cores (8-core to 76-core)
# - RAM amount (8GB to 128GB+)
# - Thermal design (mobile vs desktop)

# For an M3 Ultra with 128GB RAM, Maximum Performance mode provides:
# - 16 concurrent file processing
# - 64-item batch sizes
# - large-v3 Whisper model
# - MPS acceleration for optimal throughput
# - 5-10x faster processing than default settings

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# If you experience issues:
# 1. Start with "balanced" profile
# 2. Check logs for memory/thermal warnings
# 3. Reduce batch_size if you get out-of-memory errors
# 4. Enable sequential_processing for problematic files
# 5. Use "battery_saver" profile for maximum stability

# =============================================================================
# 128K CONTEXT MODELS FOR LARGE DOCUMENTS
# =============================================================================

# For documents larger than 50,000 tokens (like 204K character transcripts):

# RECOMMENDED: phi3:mini-128k
# - 3.8B parameters, 128K context window
# - Optimized for Apple M3 silicon 
# - Excellent summarization quality
# - Memory efficient (~4GB RAM)
# - Suggested max_tokens: 15,000-20,000

# ALTERNATIVE: llama3.1:8b-instruct  
# - 8B parameters, 128K context window
# - Good performance on Apple Silicon
# - Strong reasoning capabilities
# - Suggested max_tokens: 10,000-15,000

# ALTERNATIVE: mistral:7b-instruct-v0.3
# - 7B parameters, 128K context window  
# - Excellent for summarization tasks
# - Good balance of speed and quality
# - Suggested max_tokens: 10,000-15,000

# To use these models:
# 1. Set local_config.model to one of the above (or select in GUI)
# 2. Increase max_tokens to 15,000-20,000 for detailed summaries
# 3. Use "local" provider in GUI summarization settings
# 4. Download the model via: ollama pull phi3:mini-128k 