# Knowledge System Configuration Template
# Copy this file to settings.yaml and customize for your needs

# Application Settings
app:
  name: "Knowledge System"
  version: "0.1.1"
  debug: false

# File Paths
paths:
  data_dir: "~/Documents/KnowledgeSystem"
  output_dir: "~/Documents/KnowledgeSystem/output"
  cache_dir: "~/Documents/KnowledgeSystem/cache"
  logs_dir: "./logs"

# Performance & Hardware Optimization
performance:
  # Performance profile selection
  # Options: auto, battery_saver, balanced, high_performance, maximum_performance
  profile: "auto"

  # Hardware detection settings
  enable_hardware_detection: true

  # Override settings (leave null to use profile defaults)
  override_whisper_model: null  # Options: tiny, base, small, medium, large, large-v3
  override_device: null         # Options: auto, cpu, cuda, mps
  override_batch_size: null     # Number (8-64)
  override_max_concurrent: null # Number (1-16)

  # Use case optimization
  # Options: general, large_batch, mobile
  optimize_for_use_case: "general"

  # Force specific hardware capabilities (advanced users only)
  force_mps: false      # Force MPS acceleration on Apple Silicon
  force_coreml: false   # Force CoreML acceleration on Apple Silicon

# Thread Management & Resource Control
thread_management:
  # OpenMP thread count for transcription processing
  # Default: min(8, cpu_count) - adjust based on your system
  omp_num_threads: 8

  # Maximum number of files to process simultaneously
  # Default: min(4, cpu_count // 2) - prevents resource contention
  max_concurrent_files: 4

  # Thread limit per individual process
  # Default: min(4, cpu_count // 2) - controls per-process resources
  per_process_thread_limit: 4

  # Enable/disable parallel processing of multiple files
  enable_parallel_processing: true

  # Force sequential processing to avoid all resource conflicts
  # Set to true if you experience truncation or memory issues
  sequential_processing: false

  # Enable tokenizer parallelism (may show warnings but can improve performance)
  tokenizers_parallelism: false

  # Enable MPS fallback for PyTorch on macOS
  pytorch_enable_mps_fallback: true

# Transcription Settings
transcription:
  whisper_model: "medium"  # tiny, base, small, medium, large - medium recommended for balance of speed/accuracy
  device: "auto"  # auto (detect best), cpu, cuda (NVIDIA GPU), mps (Apple Silicon GPU)
  use_gpu: true  # Deprecated - use device instead
  diarization: false
  min_words: 50
  use_whisper_cpp: false  # Use whisper.cpp with Core ML acceleration on macOS

# Speaker Identification & Attribution Settings
# Controls how speakers are identified and attributed in transcripts
speaker_identification:
  # Diarization sensitivity mode
  # Options: bredin, dialogue, aggressive, balanced, conservative
  # - bredin: Hervé Bredin's 2022 challenge winning config (RECOMMENDED for podcasts)
  # - dialogue: Best for interviews, podcasts with quick exchanges
  # - aggressive: Capture more speakers, more sensitive to changes
  # - balanced: Middle ground for general use
  # - conservative: Fewer speakers, merge similar voices
  diarization_sensitivity: "bredin"

  # Oracle mode: Force specific number of speakers (null = auto-detect)
  # For 2-person podcasts, setting num_speakers: 2 greatly improves accuracy
  num_speakers: 2

  # Bredin-style tuned pyannote hyperparameters
  # Tune these using scripts/tune_diarization.py on your own labeled data
  # Default values are from Bredin's 2022 diarization challenge winning config
  clustering_threshold: 0.70      # Speaker clustering similarity threshold
  min_cluster_size: 12            # Minimum embeddings per speaker cluster
  min_duration_off: 0.0           # Allow all pauses to potentially split speech

  # Word-driven alignment settings
  # Speaker attribution uses DTW word-level timestamps + median filtering
  median_filter_window: 5         # Smooth speaker labels over N words to reduce flips

  # Persistent speaker profiles (FIXED: now uses stable regions only)
  # Stores voice fingerprints for recurring hosts across episodes
  # Fingerprints are extracted only from stable speaker regions (2+ seconds)
  # After 3-5 episodes, achieves 97%+ accuracy for known speakers
  persistent_profiles:
    enabled: true
    stable_region_min_duration: 2.0   # Only fingerprint regions where speaker is stable for N seconds
    profile_accumulation: true        # Build better profiles over time with each episode

# LLM Settings
llm:
  provider: "openai"  # openai, claude, local
  model: "gpt-4"
  max_tokens: 10000
  temperature: 0.1

# Local LLM Settings (for Ollama/LM Studio)
local_config:
  base_url: "http://localhost:11434"
  model: "qwen2.5:72b-instruct-q6_K"    # For large documents, consider: phi3:mini-128k (128K context)
  max_tokens: 10000                      # For 128K models, you can increase to 15000-20000
  temperature: 0.1

# API Keys (set these in your environment or directly here)
api_keys:
  openai_api_key: "${OPENAI_API_KEY}"
  anthropic_api_key: "${ANTHROPIC_API_KEY}"
  youtube_api_key: "${YOUTUBE_API_KEY}"
  # WebShare removed; use Bright Data only
  huggingface_token: "${HUGGINGFACE_HUB_TOKEN}"  # Required for pyannote diarization

# Processing Options
processing:
  batch_size: 10
  concurrent_jobs: 2
  retry_attempts: 3
  timeout_seconds: 300

  # Process Tab Defaults
  default_transcribe: true      # Enable transcription by default
  default_summarize: true        # Enable summarization by default

# File Watcher / Monitor Tab Defaults
file_watcher:
  default_file_patterns: "*.mp4,*.mp3,*.wav,*.m4a,*.pdf,*.txt,*.md"
  default_debounce_delay: 5      # seconds
  default_recursive: true         # Watch subdirectories
  default_auto_process: true      # Auto-process new files
  default_system2_pipeline: false # Don't use System 2 pipeline by default

# YouTube Processing Options
youtube_processing:
  # Proxy strict mode - protects your IP from YouTube rate limits/bans
  # When true: Blocks YouTube operations if proxy fails (RECOMMENDED for account safety)
  # When false: Falls back to direct connection if proxy fails (RISKY - may expose your IP)
  proxy_strict_mode: true

  # Automatically disable delays when rotating proxies are available
  disable_delays_with_proxy: false

  # Whether to use delays when proxies are configured
  use_proxy_delays: true

  # Rate limiting delays (in seconds)
  metadata_delay_min: 0.5
  metadata_delay_max: 2.0
  transcript_delay_min: 1.0
  transcript_delay_max: 3.0
  api_batch_delay_min: 1.0
  api_batch_delay_max: 3.0

  # Session-based download strategy (advanced anti-bot detection)
  # Based on research: duty-cycle (periodic inactivity) > micro-sleeps for avoiding detection
  enable_session_based_downloads: true  # Enable session-based strategy with duty cycles

  # Session scheduling (2-4 sessions/day, each 60-180 min)
  sessions_per_day_min: 2
  sessions_per_day_max: 4
  session_duration_min: 60   # minutes
  session_duration_max: 180  # minutes
  max_downloads_per_session_min: 100
  max_downloads_per_session_max: 250

  # yt-dlp rate limiting and jitter (prevents "robotic" traffic patterns)
  rate_limit_min_mbps: 0.8   # Minimum download rate in MB/s
  rate_limit_max_mbps: 1.5   # Maximum download rate in MB/s (randomized per session)
  concurrent_downloads_min: 1  # Minimum concurrent downloads (1-2 recommended)
  concurrent_downloads_max: 2  # Maximum concurrent downloads

  # Jitter between files and requests (smooths edges, reduces robotic cadence)
  sleep_interval_min: 8      # Minimum sleep between files in seconds
  sleep_interval_max: 25     # Maximum sleep between files in seconds
  sleep_requests: 0.8        # Sleep between HTTP requests in seconds

  # Automatic cooldown on rate limiting (429/403 errors)
  enable_auto_cooldown: true
  cooldown_min_minutes: 45   # Minimum cooldown period when rate limited
  cooldown_max_minutes: 180  # Maximum cooldown period (randomized)

  # URL shuffling (prevents sequential hammering of single channel/playlist)
  shuffle_urls: true

  # Download archive (prevents re-downloading already processed videos)
  use_download_archive: true
  download_archive_path: "~/.knowledge_system/youtube_downloads.txt"

# Podcast Discovery (YouTube-to-RSS Mapping)
podcast_discovery:
  # Enable automatic mapping of YouTube URLs to native podcast RSS feeds
  # When enabled, the system will try to find podcast RSS feeds for YouTube videos
  # This bypasses YouTube rate limiting entirely (RSS feeds have no rate limits)
  enable_youtube_to_rss_mapping: true

  # API keys for podcast discovery services (all optional)
  # PodcastIndex.org: Free tier available, get key from https://api.podcastindex.org/
  podcast_index_api_key: null

  # ListenNotes.com: Free tier available, get key from https://www.listennotes.com/api/
  listen_notes_api_key: null

  # Cache mappings to avoid repeated API calls
  cache_mappings: true
  mapping_cache_path: "~/.knowledge_system/podcast_mappings.json"

# Maps of Content (MOC) Generation
moc:
  default_theme: "topical"  # topical, chronological, hierarchical
  default_depth: 2

  # Content extraction settings
  extract_people: true
  extract_tags: true
  extract_mental_models: true
  extract_jargon: true
  extract_beliefs: true

  # Minimum thresholds
  min_people_mentions: 2
  min_tag_occurrences: 3
  min_belief_confidence: 0.7

# Monitoring & Logging
monitoring:
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file_max_size: 10  # MB
  log_file_backup_count: 5

  # Performance monitoring
  enable_performance_tracking: true
  track_processing_times: true
  track_resource_usage: true

# GUI Feature Toggles
gui_features:
  # Tab visibility settings
  show_process_management_tab: false  # Set to true to enable Process Management tab in GUI
  show_file_watcher_tab: true         # File Watcher tab (default: enabled)

  # Advanced feature toggles
  enable_advanced_features: false     # Enable experimental/advanced GUI features

# Cloud Services Configuration
cloud:
  # Supabase settings for cloud sync and storage (optional)
  supabase_url: null    # Your Supabase project URL (e.g., "https://your-project.supabase.co")
  supabase_key: null    # Your Supabase service role or anon key
  supabase_bucket: null # Default storage bucket name (e.g., "knowledge-files")

# Summarization Settings (inherits from llm by default)
summarization:
  provider: "openai"
  model: "gpt-4"
  max_tokens: 10000  # Increased for better summaries
  temperature: 0.1

# GetReceipts Integration Settings
getreceipts:
  # Enable GetReceipts integration
  enabled: false

  # GetReceipts API settings
  base_url: "https://getreceipts-web.vercel.app"
  timeout_seconds: 30
  max_retries: 3
  retry_delay: 1.0

  # Export filtering settings
  min_confidence: 0.6          # Only export claims above this confidence
  max_claims_per_export: 20    # Limit claims per session to avoid spam
  include_all_tiers: true      # Export all claim tiers (A, B, C)
  include_tier_c: true         # Include Tier C claims (set false for only A+B)
  include_evidence_timestamps: true  # Include YouTube timestamp links

  # Auto-export settings
  auto_export_after_processing: false  # Auto-export after successful HCE processing
  export_only_on_success: true         # Only export if processing was successful

# =============================================================================
# PROCESSING MODE CONFIGURATION
# =============================================================================
# Control how claim extraction is processed: real-time, batch, or auto

batch_processing:
  # Mode: "realtime" | "batch" | "auto"
  # - realtime: Process immediately using configured LLM (local or cloud)
  # - batch: Use batch APIs (50% discount, 24-48hr processing)
  # - auto: Automatically switch based on volume
  mode: "auto"
  
  # Auto mode threshold: switch to batch when segments > this
  auto_batch_threshold: 100

  # Provider and model for batch stages
  batch_provider: "openai"              # openai or anthropic
  batch_mining_model: "gpt-5-mini"      # Model for claim extraction
  batch_flagship_model: "gpt-5-mini"    # Model for claim evaluation
  
  # Re-mining uses stronger model (can be different provider)
  batch_remine_provider: "anthropic"
  batch_remine_model: "claude-3.7-sonnet"
  
  # Real-time model configuration (when mode is "realtime" or "auto" stays realtime)
  realtime_mining_model: "ollama://qwen2.5:72b-instruct"
  realtime_flagship_model: "ollama://qwen2.5:72b-instruct"
  
  # Re-mining triggers
  remine_enabled: true
  remine_confidence_threshold: 4.0      # Re-mine if avg confidence < 4
  remine_empty_segments: true           # Re-mine segments with 0 claims
  remine_max_percent: 15                # Cap at 15% of segments
  
  # Prompt caching optimization (OpenAI)
  # Caching can provide additional 25% savings on input tokens
  enable_cache_optimization: true
  sequential_batch_submission: true     # Submit batches one at a time for cache warmup
  batch_delay_seconds: 30               # Wait between batches for cache warmup
  
  # Polling settings
  poll_interval_seconds: 60             # How often to check batch status
  max_requests_per_batch: 10000         # Split larger jobs into batches

# =============================================================================
# BATCH PROCESSING COST ESTIMATES
# =============================================================================
# 
# For 5,000 hours of podcast audio (~90,000 segments):
#
# Real-time processing (no batch):
#   - OpenAI GPT-5 Mini: ~$438
#   - Local Qwen 72B: ~$75 electricity, 10-35 days processing
#
# Batch processing (50% discount):
#   - Mining (GPT-5 Mini): ~$104
#   - Flagship Evaluation: ~$20
#   - Re-mining 10% (Claude Sonnet): ~$95
#   - TOTAL: ~$219
#
# Batch + Prompt Caching (additional 25% input savings):
#   - TOTAL: ~$170-195
#
# Processing time: 48-72 hours for batch mode
# =============================================================================

# =============================================================================
# PERFORMANCE PROFILES EXPLANATION
# =============================================================================

# The Knowledge System automatically detects your hardware and optimizes
# performance based on your Mac's capabilities. Here's what each profile does:

# AUTO (Recommended):
# - Automatically selects the best profile for your hardware
# - M3 Ultra with 128GB RAM → Maximum Performance
# - M2 Pro with 32GB RAM → High Performance
# - M1 with 16GB RAM → Balanced

# BATTERY_SAVER:
# - Minimal resource usage for laptop/mobile use
# - Sequential processing, single file at a time
# - Uses "tiny" Whisper model for speed
# - Best for: MacBook Air, preserving battery life

# BALANCED (Default):
# - Good balance of performance and resource usage
# - Moderate parallel processing
# - Uses "base" Whisper model
# - Best for: Most users, general-purpose work

# HIGH_PERFORMANCE:
# - Maximizes performance within thermal limits
# - High parallel processing
# - Uses recommended model for your hardware
# - Best for: Mac Studio, iMac Pro, heavy workloads

# MAXIMUM_PERFORMANCE:
# - Pushes hardware to absolute limits
# - Maximum parallel processing and batch sizes
# - Uses largest supported Whisper model
# - Prioritizes MPS over CoreML for large batches
# - Best for: M3 Ultra, M2 Ultra, professional use

# =============================================================================
# HARDWARE-SPECIFIC OPTIMIZATIONS
# =============================================================================

# The system automatically detects and optimizes for:
# - Chip type (M1, M2, M3 variants)
# - CPU cores (8-core to 24-core)
# - GPU cores (8-core to 76-core)
# - RAM amount (8GB to 128GB+)
# - Thermal design (mobile vs desktop)

# For an M3 Ultra with 128GB RAM, Maximum Performance mode provides:
# - 16 concurrent file processing
# - 64-item batch sizes
# - large-v3 Whisper model
# - MPS acceleration for optimal throughput
# - 5-10x faster processing than default settings

# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# If you experience issues:
# 1. Start with "balanced" profile
# 2. Check logs for memory/thermal warnings
# 3. Reduce batch_size if you get out-of-memory errors
# 4. Enable sequential_processing for problematic files
# 5. Use "battery_saver" profile for maximum stability

# =============================================================================
# 128K CONTEXT MODELS FOR LARGE DOCUMENTS
# =============================================================================

# For documents larger than 50,000 tokens (like 204K character transcripts):

# RECOMMENDED: phi3:mini-128k
# - 3.8B parameters, 128K context window
# - Optimized for Apple M3 silicon
# - Excellent summarization quality
# - Memory efficient (~4GB RAM)
# - Suggested max_tokens: 15,000-20,000

# ALTERNATIVE: llama3.1:8b-instruct
# - 8B parameters, 128K context window
# - Good performance on Apple Silicon
# - Strong reasoning capabilities
# - Suggested max_tokens: 10,000-15,000

# ALTERNATIVE: mistral:7b-instruct-v0.3
# - 7B parameters, 128K context window
# - Excellent for summarization tasks
# - Good balance of speed and quality
# - Suggested max_tokens: 10,000-15,000

# To use these models:
# 1. Set local_config.model to one of the above (or select in GUI)
# 2. Increase max_tokens to 15,000-20,000 for detailed summaries
# 3. Use "local" provider in GUI summarization settings
# 4. Download the model via: ollama pull phi3:mini-128k
