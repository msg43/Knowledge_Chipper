# Multi-Tiered Speaker Identification System Summary & Stress Test Guide

## System Overview

The Knowledge Chipper uses a **6-stage multi-tiered pipeline** to identify speakers and assign proper names. This system combines audio analysis, voice fingerprinting, LLM intelligence, and user confirmation to achieve 85-95% accuracy.

---

## The 6-Stage Pipeline

### Stage 1: Audio Diarization (pyannote.audio v3.1)
**Purpose**: Separate different voices in audio  
**Tool**: Deep learning models (speechbrain, transformers)  
**Output**: Generic speaker IDs (`SPEAKER_00`, `SPEAKER_01`, etc.)  
**Settings**:
- **Conservative mode** (default): `min_cluster_size=20`, `threshold=0.75`, `min_duration_on=1.0`
- **Adaptive clustering**: Automatically adjusts `min_cluster_size` for short videos (< 5 min)
- **Device support**: CUDA (GPU), MPS (Apple Silicon), CPU fallback

**Key Files**:
- `src/knowledge_system/processors/diarization.py`
- `src/knowledge_system/processors/audio_processor.py`

**Status**: ‚úÖ **ACTIVE** - Always runs when diarization is enabled

---

### Stage 2: Voice Fingerprinting (Two-Tier System)
**Purpose**: Fix over-segmentation by merging speakers with the same voice

#### Tier 1: Audio-Based Voice Fingerprinting (Primary)
**Method**: Multi-modal voice feature extraction
- **MFCC** (Mel-frequency cepstral coefficients) - 20% weight
- **Spectral features** (centroid, rolloff, zero crossing) - 10% weight  
- **Prosodic features** (pitch, tempo, rhythm) - 10% weight
- **Wav2Vec2 embeddings** (deep learning) - 30% weight
- **ECAPA-TDNN embeddings** (speaker verification) - 30% weight

**Threshold**: 0.8 (80% similarity = same person) - **Conservative to preserve speakers**

**Key Files**:
- `src/knowledge_system/voice/voice_fingerprinting.py`
- `src/knowledge_system/processors/speaker_processor.py` ‚Üí `_voice_fingerprint_merge_speakers()`

**Status**: ‚úÖ **ACTIVE** - Runs automatically when audio path is available

#### Tier 2: Text-Based Heuristics (Fallback)
**Method**: Compares speakers based on timing patterns and text similarity
- Temporal overlap analysis
- Duration ratio comparison
- Segment count ratio
- **Threshold**: 0.85 (85% similarity) - **Very conservative**

**When Tier 2 is Used**:
1. **Insufficient valid audio segments** (most common): One speaker has segments too short/invalid for fingerprinting, so Tier 1 can't compare (needs 2+ fingerprints)
2. **Audio path not provided**: Reprocessing speaker assignments from existing transcript, or audio file deleted after transcription
3. **Audio loading fails**: Transcription succeeded but voice fingerprinting library fails (rare)

**Note**: If audio is corrupted/unsupported, transcription fails BEFORE speaker processing, so fallback wouldn't help in that scenario.

**Key Files**:
- `src/knowledge_system/processors/speaker_processor.py` ‚Üí `_voice_fingerprint_merge_speakers_fallback()`

**Status**: ‚úÖ **ACTIVE** - Automatically falls back when Tier 1 can't compare speakers

---

### Stage 3: Heuristic Over-Segmentation Detection
**Purpose**: Additional safety net for merging similar speakers  
**Method**: Pattern-based analysis of speaker segments  
**Key Files**:
- `src/knowledge_system/processors/speaker_processor.py` ‚Üí `_detect_and_merge_oversegmented_speakers()`

**Status**: ‚úÖ **ACTIVE** - Runs after voice fingerprinting

---

### Stage 4: LLM-Based Name Suggestion
**Purpose**: Assign real names to speakers (not just IDs)

**Process**:
1. **Channel Mapping** (Priority 1): Checks database for known hosts by channel name
2. **Metadata Analysis**: Extracts names from title, description, channel
3. **Transcript Analysis**: Examines first 5 segments per speaker
4. **Regex Pattern Matching**: Finds self-introductions (`"I'm [Name]"`, `"My name is [Name]"`)
5. **Phonetic Matching**: Handles transcription errors (e.g., "Stacy Rasgon" vs "Stacey Raskin")
6. **Context Inference**: Infers names from roles, topics, or characteristics

**Critical Rules**:
- ‚úÖ No empty names allowed
- ‚úÖ No generic labels ("Speaker 1", "Speaker 2")
- ‚úÖ Each speaker gets unique name
- ‚úÖ Metadata names beat transcription variants
- ‚úÖ Skeptically evaluates if multiple IDs are actually the same person

**Key Files**:
- `src/knowledge_system/utils/llm_speaker_suggester.py`
- `src/knowledge_system/processors/speaker_processor.py` ‚Üí `_suggest_all_speaker_names_together()`

**Status**: ‚úÖ **ACTIVE** - Mandatory step, uses OpenAI/Anthropic/local MVP LLM

---

### Stage 5: User Confirmation (GUI Mode)
**Purpose**: Allow user to verify/correct AI suggestions  
**Tool**: Custom Qt dialog  
**Features**:
- Shows AI-suggested names with confidence scores
- Displays sample speech segments
- User can accept, edit, or override names
- Assignments saved to database

**Key Files**:
- `src/knowledge_system/gui/dialogs/speaker_assignment_dialog.py`

**Status**: ‚úÖ **ACTIVE** - Runs in GUI mode only

---

### Stage 6: Database Persistence
**Purpose**: Store assignments and voice fingerprints for future use

**Storage**:
1. **Speaker Assignments** (`speaker_assignments` table)
   - Maps speaker IDs ‚Üí assigned names
   - Tracks confidence scores and sources
   - Persists across sessions

2. **Voice Fingerprints** (`speaker_voices` table)
   - Stores multi-modal voice embeddings
   - Enables cross-recording speaker recognition
   - Can match speakers across different episodes

**Key Files**:
- `src/knowledge_system/database/speaker_models.py`
- `src/knowledge_system/database/speaker_voice_models.py`

**Status**: ‚úÖ **ACTIVE** - All assignments and fingerprints are persisted

---

## Complete Pipeline Flow

```
Audio File
    ‚Üì
[Stage 1: Diarization] 
    ‚Üí Generic IDs: SPEAKER_00, SPEAKER_01, SPEAKER_02
    ‚Üì
[Stage 2: Voice Fingerprinting - Tier 1 (Audio)]
    ‚Üí Extracts multi-modal fingerprints
    ‚Üí Compares similarity (threshold: 0.8)
    ‚Üí Merges: SPEAKER_00 + SPEAKER_02 = SPEAKER_00
    ‚Üí Result: SPEAKER_00, SPEAKER_01
    ‚Üì
[Stage 2: Voice Fingerprinting - Tier 2 (Text Fallback)]
    ‚Üí Only if audio analysis fails
    ‚Üí Uses timing/text patterns (threshold: 0.85)
    ‚Üì
[Stage 3: Heuristic Merging]
    ‚Üí Additional pattern-based merging
    ‚Üì
[Stage 4: LLM Name Suggestion]
    ‚Üí Channel mapping ‚Üí Metadata ‚Üí Transcript ‚Üí Regex ‚Üí Inference
    ‚Üí Result: "Jeff Snider" (0.8), "Andrew" (0.7)
    ‚Üì
[Stage 5: User Confirmation] (GUI only)
    ‚Üí User verifies/edits names
    ‚Üì
[Stage 6: Database Storage]
    ‚Üí Saves assignments + voice fingerprints
    ‚Üì
Final Transcript
    ‚Üí Segments with real speaker names
```

---

## System Status Verification

### ‚úÖ All Systems Confirmed Active

1. **Diarization**: ‚úÖ Enabled by default, runs automatically
   - Configuration: `diarization_sensitivity: "conservative"` (in `config.py`)
   - Adaptive clustering: ‚úÖ Active for short videos

2. **Voice Fingerprinting (Audio)**: ‚úÖ Active
   - Default enabled: `voice_fingerprinting_enabled: true` (in `config.py`)
   - Called automatically in `prepare_speaker_data()` when audio_path provided
   - Threshold: 0.8 (conservative)

3. **Voice Fingerprinting (Text Fallback)**: ‚úÖ Active
   - Automatically falls back when audio unavailable
   - Threshold: 0.85 (very conservative)

4. **Heuristic Merging**: ‚úÖ Active
   - Runs after voice fingerprinting
   - Additional safety net

5. **LLM Name Suggestion**: ‚úÖ Active
   - Mandatory step in `_suggest_all_speaker_names_together()`
   - Uses configured LLM provider (OpenAI/Anthropic/local)

6. **Database Persistence**: ‚úÖ Active
   - All assignments saved to `speaker_assignments` table
   - Voice fingerprints saved to `speaker_voices` table

---

## Stress Test Plan

### Test 1: Short Monologue (3-5 minutes)
**Purpose**: Verify adaptive clustering prevents over-segmentation

**Test File**: Single-speaker monologue video (3-5 minutes)

**Expected Behavior**:
- ‚úÖ Diarization uses adaptive `min_cluster_size` (6-10 for 3 min video)
- ‚úÖ Voice fingerprinting merges any false splits
- ‚úÖ Final result: 1 speaker only
- ‚úÖ LLM assigns proper name (not "Speaker 1")

**Verification**:
```bash
# Check logs for:
- "üîß Short video detected (X.X min): Using adaptive min_cluster_size=Y"
- "‚úÖ Voice fingerprinting merged X speaker(s)"
- "üîí PRESERVATION: All X speaker(s) preserved"
```

**Success Criteria**:
- Only 1 speaker detected
- Proper name assigned (not generic)
- No "Voice fingerprinting did NOT merge speakers" warnings

---

### Test 2: Two-Person Interview (10-15 minutes)
**Purpose**: Verify system correctly separates and names two distinct speakers

**Test File**: Interview with clear host + guest

**Expected Behavior**:
- ‚úÖ Diarization detects 2 speakers
- ‚úÖ Voice fingerprinting does NOT merge (different voices)
- ‚úÖ LLM identifies both names from metadata/transcript
- ‚úÖ Both speakers get unique, proper names

**Verification**:
```bash
# Check logs for:
- "üîç Voice similarity: SPEAKER_00 vs SPEAKER_01 = X.XXX"
- "üîí PRESERVATION: Not merging SPEAKER_00 and SPEAKER_01 (similarity: X.XXX <= 0.8)"
- "‚úÖ Extracted fingerprint for SPEAKER_XX from Y segments"
```

**Success Criteria**:
- Exactly 2 speakers detected
- Both get proper names (not "Speaker 1", "Speaker 2")
- Similarity score < 0.8 between speakers
- Names match metadata/transcript

---

### Test 3: Over-Segmented Monologue
**Purpose**: Verify voice fingerprinting merges false splits

**Test File**: Single speaker but diarization creates 2-3 IDs

**Expected Behavior**:
- ‚úÖ Diarization initially detects 2-3 speakers
- ‚úÖ Voice fingerprinting calculates high similarity (>0.8)
- ‚úÖ Speakers merged into 1
- ‚úÖ Final result: 1 speaker with proper name

**Verification**:
```bash
# Check logs for:
- "üîç Voice similarity: SPEAKER_00 vs SPEAKER_01 = 0.XXX (threshold: 0.8, will_merge: True)"
- "üîó Voice fingerprinting: SPEAKER_00 and SPEAKER_01 are likely the same speaker"
- "üéØ Merged SPEAKER_01 into SPEAKER_00 (voice similarity: 0.XXX)"
- "‚úÖ Voice fingerprinting merged 1 speaker(s): 2 ‚Üí 1"
```

**Success Criteria**:
- Initial: 2-3 speakers detected
- Final: 1 speaker after merging
- Similarity score > 0.8 between merged speakers
- Proper name assigned

---

### Test 4: Audio Unavailable Fallback
**Purpose**: Verify text-based fallback works

**Test File**: Any video, but disable audio path

**Expected Behavior**:
- ‚úÖ Voice fingerprinting detects no audio path
- ‚úÖ Falls back to text-based heuristics
- ‚úÖ Uses conservative threshold (0.85)
- ‚úÖ Still attempts to merge if appropriate

**Verification**:
```bash
# Check logs for:
- "üîç DIAGNOSTIC: No audio path provided - using text-based similarity fallback"
- "üîí PRESERVATION: Text-based merging using conservative threshold: 0.85"
- "üîç Text-based similarity: SPEAKER_00 vs SPEAKER_01 = X.XXX"
```

**Success Criteria**:
- System gracefully handles missing audio
- Text-based merging attempts made
- No crashes or errors

---

### Test 5: LLM Name Suggestion
**Purpose**: Verify LLM correctly identifies names

**Test File**: Video with clear metadata (title/description with names)

**Expected Behavior**:
- ‚úÖ LLM analyzes metadata + transcript
- ‚úÖ Finds names in title/description
- ‚úÖ Matches to speakers via content analysis
- ‚úÖ Assigns proper names (not generic)

**Verification**:
```bash
# Check logs for:
- "üì∫ Channel has known hosts: ['Name1', 'Name2']"
- "üîç LLM analyzing speakers..."
- "‚úÖ LLM suggested names: {'SPEAKER_00': ('Name1', 0.8), ...}"
```

**Success Criteria**:
- Names extracted from metadata
- Names match actual speakers
- Confidence scores > 0.6
- No generic names ("Speaker 1", etc.)

---

### Test 6: Database Persistence
**Purpose**: Verify assignments are saved

**Test File**: Any processed video

**Expected Behavior**:
- ‚úÖ Speaker assignments saved to database
- ‚úÖ Voice fingerprints stored (if audio available)
- ‚úÖ Can retrieve assignments for future use

**Verification**:
```python
# Check database:
from knowledge_system.database.speaker_models import get_speaker_db_service

db_service = get_speaker_db_service()
assignments = db_service.get_speaker_assignments_for_recording(recording_path)
voices = db_service.get_all_voices()

assert len(assignments) > 0  # Assignments saved
assert len(voices) > 0  # Fingerprints saved (if audio available)
```

**Success Criteria**:
- Assignments persist in database
- Voice fingerprints stored (when available)
- Can retrieve for future recordings

---

## Running the Stress Tests

### Quick Test Script

Create `test_speaker_system.py`:

```python
#!/usr/bin/env python3
"""Stress test for speaker identification system."""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from knowledge_system.processors.audio_processor import AudioProcessor
from knowledge_system.logger import get_logger

logger = get_logger(__name__)

def test_speaker_system(audio_file: Path):
    """Test complete speaker identification pipeline."""
    logger.info(f"üß™ Testing speaker system with: {audio_file}")
    
    processor = AudioProcessor(
        enable_diarization=True,
        db_service=None,  # Add if you have DB
    )
    
    result = processor.process(audio_file, diarization=True)
    
    if result.success:
        logger.info("‚úÖ Processing succeeded")
        
        # Check transcript for speaker assignments
        transcript = result.data.get("transcript", {})
        segments = transcript.get("segments", [])
        
        speakers = set()
        for seg in segments:
            speaker = seg.get("speaker")
            if speaker:
                speakers.add(speaker)
        
        logger.info(f"üìä Detected {len(speakers)} speaker(s): {speakers}")
        
        # Verify no generic names
        generic_names = {"Speaker 1", "Speaker 2", "Unknown", "UNKNOWN"}
        if speakers & generic_names:
            logger.warning(f"‚ö†Ô∏è Generic names detected: {speakers & generic_names}")
        else:
            logger.info("‚úÖ No generic names - all proper names assigned")
        
        return True
    else:
        logger.error(f"‚ùå Processing failed: {result.errors}")
        return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python test_speaker_system.py <audio_file>")
        sys.exit(1)
    
    audio_file = Path(sys.argv[1])
    if not audio_file.exists():
        print(f"Error: File not found: {audio_file}")
        sys.exit(1)
    
    success = test_speaker_system(audio_file)
    sys.exit(0 if success else 1)
```

### Manual Verification Checklist

For each test, verify:

- [ ] **Diarization runs**: Check logs for "Starting diarization processing"
- [ ] **Adaptive clustering active** (short videos): Check for "Short video detected" message
- [ ] **Voice fingerprinting runs**: Check for "Voice fingerprinting available" or "Falling back to text-based"
- [ ] **Similarity scores logged**: Check for "Voice similarity: SPEAKER_X vs SPEAKER_Y = X.XXX"
- [ ] **Merging occurs** (if appropriate): Check for "Merged SPEAKER_X into SPEAKER_Y"
- [ ] **LLM suggestions made**: Check for "LLM suggested names" or "Channel has known hosts"
- [ ] **Proper names assigned**: Verify no "Speaker 1", "Speaker 2" in final output
- [ ] **Database persistence**: Check database for saved assignments

---

## Expected Log Patterns

### Successful Monologue Processing:
```
‚úÖ Starting diarization processing
üîß Short video detected (3.2 min): Using adaptive min_cluster_size=6
‚úÖ Extracted fingerprint for SPEAKER_00 from 5 segments
üîç Voice similarity: SPEAKER_00 vs SPEAKER_01 = 0.856 (threshold: 0.8, will_merge: True)
üéØ Merged SPEAKER_01 into SPEAKER_00 (voice similarity: 0.856)
‚úÖ Voice fingerprinting merged 1 speaker(s): 2 ‚Üí 1
üì∫ Channel has known hosts: ['Jeff Snider']
‚úÖ LLM suggested names: {'SPEAKER_00': ('Jeff Snider', 0.85)}
üîí PRESERVATION: All 1 speaker(s) preserved
```

### Successful Two-Speaker Processing:
```
‚úÖ Starting diarization processing
‚úÖ Extracted fingerprint for SPEAKER_00 from 8 segments
‚úÖ Extracted fingerprint for SPEAKER_01 from 7 segments
üîç Voice similarity: SPEAKER_00 vs SPEAKER_01 = 0.423 (threshold: 0.8, will_merge: False)
üîí PRESERVATION: Not merging SPEAKER_00 and SPEAKER_01 (similarity: 0.423 <= 0.8)
‚úÖ LLM suggested names: {'SPEAKER_00': ('Host Name', 0.8), 'SPEAKER_01': ('Guest Name', 0.75)}
üîí PRESERVATION: All 2 speaker(s) preserved
```

---

## Troubleshooting

### If voice fingerprinting doesn't merge:
1. Check audio path is provided: Look for "No audio path provided" warning
2. Check audio file exists: Look for "Audio file not found" warning
3. Check similarity scores: Should be > 0.8 for merging
4. Check segment extraction: Look for "No valid audio segments" warnings

### If LLM doesn't suggest names:
1. Check LLM provider configured: Verify settings.yaml has LLM config
2. Check metadata available: Look for "Channel has known hosts" or metadata extraction
3. Check transcript quality: LLM needs readable transcript segments

### If generic names assigned:
1. Check LLM suggestions: Look for "LLM suggested names" in logs
2. Check fallback logic: May be using fallback if LLM fails
3. Check database: Previous assignments may override

---

## Summary

**All 6 stages are fully active and operational:**

1. ‚úÖ **Diarization**: Active with adaptive clustering
2. ‚úÖ **Voice Fingerprinting (Audio)**: Active (threshold: 0.8)
3. ‚úÖ **Voice Fingerprinting (Text)**: Active fallback (threshold: 0.85)
4. ‚úÖ **Heuristic Merging**: Active safety net
5. ‚úÖ **LLM Name Suggestion**: Active and mandatory
6. ‚úÖ **Database Persistence**: Active for all assignments

The system uses **conservative thresholds** (0.8 for audio, 0.85 for text) to preserve speaker content and avoid false merges. All stages include comprehensive logging for verification and debugging.

